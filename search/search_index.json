{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome!","text":"<p>This is the write-up website for CS184 spring 2024 assignments. Currently, we have Homework 1, Homework 2, and Homework 3 available. We are currently working on Homework 4 and will update the website as soon as possible.</p>"},{"location":"#how-to-use-this-website","title":"How to use this website","text":"<p>On the top of the page, you can find a navigation bar that allows you to navigate to different sections. </p> <p>On the left side, all the subpages in one section are listed and can be accessed by clicking on them. </p> <p>For each subpage, there is a table of contents on the right side that allows you to jump to different parts of the page.</p> <p>Info</p> <p>We will update this website and add more features as we progress through the course. Stay tuned!</p>"},{"location":"#group-members","title":"Group Members","text":"<ul> <li>Ruhao Tian</li> <li>Zixuan Wan</li> </ul> <p>Feel free to contact us via Github if you have any questions or suggestions!</p>"},{"location":"Homework1/Overview/","title":"Homework 1: Rasterizer Overview","text":""},{"location":"Homework1/Overview/#overview","title":"Overview","text":"<p>In this homework, we implement a rasterizer that:</p> <ul> <li>Able to rasterize triangles, points and lines</li> <li>Support supersampling</li> <li>Support multiple sampling techniques: nearest and bilinear</li> <li>Support level sampling</li> <li>Support multiple level sampling techniques: level 0, nearest and interpolation</li> </ul> <p>We also implement the translation, scaling and rotation of SVG elements, and created our own SVG robot figure.</p> <p>We build up our documentation website using markdown and GitHub Pages. Which is the site(or the PDF file) you are currently viewing. We add navigation bar, table of contents, code syntax highlighting and many other details to make the website more readable.</p> <p>You can find the detailed implementation and explanation of each task in the following pages. Happy reading!</p> Note for PDF <p>Some features of the website, such as picture lightbox, may not be available in the PDF version. We recommend reading the website directly for the best experience.</p>"},{"location":"Homework1/Task1/","title":"Task 1: Drawing Single-Color Triangles","text":""},{"location":"Homework1/Task1/#method","title":"Method","text":"<p>The high-level idea of drawing triangles comes in two steps:</p> <ol> <li> <p>Bounding Box: Find the bounding box of the triangle, which is the smallest rectangle that contains the triangle. This is done by finding the minimum and maximum x and y coordinates of the triangle.</p> </li> <li> <p>Rasterization: For each pixel in the bounding box, check if it is inside the triangle. If it is, color it with the triangle's color.</p> </li> </ol>"},{"location":"Homework1/Task1/#implementation","title":"Implementation","text":""},{"location":"Homework1/Task1/#bounding-box","title":"Bounding Box","text":"<p>Given the three vertices of the triangle, finding x limits and y limits is straightforward. However, the limits are not necessarily integers. In order to ensure the bounding box fully covers the triangle, we need to take the ceiling and floor of the limits. To elaborate, the x minimum and y minimum should be rounded down when finding the boundary pixels, vice versa.</p> C++<pre><code>    // Step 1: Find the bounding box of the triangle\n    // The bounding box should at least cover the triangle, so for the left side, we take the floor of the minimum x coordinate of the triangle\n    int bounding_box_x_min = (int)floor(min(x0, min(x1, x2)));\n    // For the right side, we take the ceiling of the maximum x coordinate of the triangle\n    int bounding_box_x_max = (int)ceil(max(x0, max(x1, x2)));\n    // The same implementation for the y coordinate--floor for the bottom and ceiling for the top\n    int bounding_box_y_min = (int)floor(min(y0, min(y1, y2)));\n    int bounding_box_y_max = (int)ceil(max(y0, max(y1, y2)));\n</code></pre>"},{"location":"Homework1/Task1/#rasterization","title":"Rasterization","text":"<p>Implicit line equations are implemented to determine whether a sample position is inside or on the edge of the triangle:</p> \\[ L(x,y) = -(x - x_0)dX + (y - y_0)dY \\] <p>If \\(P_0(x_0,y_0)\\) and \\(P_1(x_1,y_1)\\) are two points on the line, then \\(dX = x_1 - x_0\\), \\(dY = y_1 - y_0\\).</p> <p>If \\(L\\) yields all positive results for the three edge, then the sample position is considered inside the triangle. When \\(L = 0\\), the sample position is on the edge. Both situations the sample position should be colored.</p> <p>Through the above method, each pixel's color could be determined through a one-time iteration.</p> C++<pre><code>    // Step 2: Iterate through all the pixels in the bounding box\n    // Initializing variables\n    int current_x_position, current_y_position;\n    float line01_delta_x = x1 - x0;\n    float line01_delta_y = y1 - y0;\n    float line12_delta_x = x2 - x1;\n    float line12_delta_y = y2 - y1;\n    float line20_delta_x = x0 - x2;\n    float line20_delta_y = y0 - y2;\n    float line01_result, line12_result, line20_result;\n    // iterate through lines\n    for (current_y_position = bounding_box_y_min; current_y_position &lt; bounding_box_y_max; current_y_position++)\n    {\n        // iterate through pixels in the line\n        for (current_x_position = bounding_box_x_min; current_x_position &lt; bounding_box_x_max; current_x_position++)\n        {\n          // check if the pixel is inside the triangle. If so, fill the corresponding buffer\n          // note when calculating line results, we should use the CENTER of the pixel as position\n          line01_result = - ((float)current_x_position + 0.5 - x0) * line01_delta_y + ((float)current_y_position + 0.5 - y0) * line01_delta_x;\n          line12_result = - ((float)current_x_position + 0.5 - x1) * line12_delta_y + ((float)current_y_position + 0.5 - y1) * line12_delta_x;\n          line20_result = - ((float)current_x_position + 0.5 - x2) * line20_delta_y + ((float)current_y_position + 0.5 - y2) * line20_delta_x;\n\n          if (line01_result &gt;= 0. &amp;&amp; line12_result &gt;= 0. &amp;&amp; line20_result &gt;= 0.)\n          {\n            fill_pixel(current_x_position,current_y_position,color);\n          }\n        }\n    }\n</code></pre>"},{"location":"Homework1/Task1/#results","title":"Results","text":"<p>The following image is the rasterized result of <code>basic/test4.svg</code>: </p>"},{"location":"Homework1/Task2/","title":"Task 2: Antialiasing by Supersampling","text":""},{"location":"Homework1/Task2/#method","title":"Method","text":"<p>In task 2, antialiasing is implemented by supersampling and downsampling. The main steps involves:</p> <ul> <li> <p>Creating and maintaining sample buffer: To store the supersampling result, a sample buffer corresponding to the sample rate is created and managed throughout the render pipeline. When sample rate updates, the sample buffer should be updated accordingly.</p> </li> <li> <p>Supersampling with sample buffer: Sampling the triangle implements the sample buffer with the same method as in task 1 except the vertice position of triangle should be transformed to the sample buffer coordinate. However, lines and points should not be antialiased. They need to be zoomed in by the sample rate before populating the sample buffer.</p> </li> <li> <p>Downsampling and populating frame buffer: Downsampling the sample buffer to the frame buffer by mixing the color of the samples corresponding to the same pixel.</p> </li> </ul>"},{"location":"Homework1/Task2/#implementation","title":"Implementation","text":""},{"location":"Homework1/Task2/#creating-and-maintaining-sample-buffer","title":"Creating and maintaining sample buffer","text":"<p>When the sample rate is updated, sample buffer should be resized. As <code>RasterizerImp::set_sample_rate</code> updates sample rate, the buffer is resized when the function is called:</p> C++<pre><code>  void RasterizerImp::set_sample_rate(unsigned int rate)\n  {\n    // TODO: Task 2: You may want to update this function for supersampling support\n\n    this-&gt;sample_rate = rate;\n\n    // updated for supersampling\n    this-&gt;sample_buffer.resize(width * height * sample_rate, Color::White);\n  }\n</code></pre> <p>The same applies to <code>RasterizerImp::set_framebuffer_target</code>:</p> C++<pre><code>  void RasterizerImp::set_framebuffer_target(unsigned char *rgb_framebuffer,\n                                             size_t width, size_t height)\n  {\n    // TODO: Task 2: You may want to update this function for supersampling support\n\n    //cout &lt;&lt; \"set_framebuffer_target\" &lt;&lt; endl;\n\n    this-&gt;width = width;\n    this-&gt;height = height;\n    this-&gt;rgb_framebuffer_target = rgb_framebuffer;\n\n    //updated for supersampling\n    this-&gt;sample_buffer.resize(width * height * sample_rate, Color::White);\n  }\n</code></pre>"},{"location":"Homework1/Task2/#supersampling-with-sample-buffer","title":"Supersampling with sample buffer","text":"<p>Define \\(zoom\\_coefficient = \\sqrt{sample\\_rate}\\), the transformation of the vertice position of triangle to the sample buffer coordinate is:</p> \\[ (x_{transformed},y_{transformed}) = (x_{original},y_{original}) \\times zoom\\_coefficient \\] C++<pre><code>    // Step 1: Convert the triangle vertices to the supersample buffer's coordinate system\n    int zoom_coefficient = sqrt(sample_rate);\n    float x0_ss = x0 * zoom_coefficient;\n    float y0_ss = y0 * zoom_coefficient;\n    float x1_ss = x1 * zoom_coefficient;\n    float y1_ss = y1 * zoom_coefficient;\n    float x2_ss = x2 * zoom_coefficient;\n    float y2_ss = y2 * zoom_coefficient;\n</code></pre> <p>The sampling process is exactly the same as in task 1, except the result fills sample buffer instead of frame buffer.</p> <p>For lines and points, they fill directly to the frame buffer pixels. This can be achieved by filling all the subpixels in the sample buffer with the same color. These subpixels form a square of size \\(zoom\\_coefficient \\times zoom\\_coefficient\\). A double loop is implemented to iterate all the subpixels.</p> C++<pre><code>  void RasterizerImp::fill_pixel(size_t x, size_t y, Color c)\n  {\n    // TODO: Task 2: You might need to this function to fix points and lines (such as the black rectangle border in test4.svg)\n    // NOTE: You are not required to implement proper supersampling for points and lines\n    // It is sufficient to use the same color for all supersamples of a pixel for points and lines (not triangles)\n\n    // Implementation by Ruhao Tian starts here\n    // As points and lines are not required to be supersampled, we can simply fill the corresponding buffer with given color\n    int zoom_coefficient = sqrt(sample_rate);\n    for (int i = 0; i &lt; zoom_coefficient; i++)\n    {\n      for (int j = 0; j &lt; zoom_coefficient; j++)\n      {\n        sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i] = c;\n      }\n    }\n  }\n</code></pre>"},{"location":"Homework1/Task2/#downsampling-and-populating-frame-buffer","title":"Downsampling and populating frame buffer","text":"<p><code>RasterizerImp::resolve_to_framebuffer</code> resolves the sample buffer to the frame buffer. It iterates all the pixels in the frame buffer. For each pixel, its subpixel square is located. </p> <p>The R, G, B values of all subpixels are averaged respectively and assigned to a temporary <code>Color</code> object <code>ds_color</code>:</p> C++<pre><code>        // for each pixel, calculate the average color of all the samples\n        ds_color.r = 0;\n        ds_color.g = 0;\n        ds_color.b = 0;\n\n        for (int i = 0; i &lt; zoom_coefficient; i++)\n        {\n          for (int j = 0; j &lt; zoom_coefficient; j++)\n          {\n            ds_color.r += sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i].r / (float)(sample_rate);\n            ds_color.g += sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i].g / (float)(sample_rate);\n            ds_color.b += sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i].b / (float)(sample_rate);\n          }\n        }\n</code></pre> <p>Then the conversion of <code>ds_color</code> to <code>rgb_framebuffer_target</code> is the same as base code:</p> C++<pre><code>        for (int k = 0; k &lt; 3; ++k)\n        {\n          this-&gt;rgb_framebuffer_target[3 * (y * width + x) + k] = (&amp;ds_color.r)[k] * 255;\n        }\n</code></pre> <p>The full implementation of <code>RasterizerImp::resolve_to_framebuffer</code> is:</p> C++<pre><code>  void RasterizerImp::resolve_to_framebuffer()\n  {\n    // TODO: Task 2: You will likely want to update this function for supersampling support\n\n    // Implementation by Ruhao Tian starts here\n\n    //cout &lt;&lt; \"resolve_to_framebuffer\" &lt;&lt; endl;\n\n    int zoom_coefficient = sqrt(sample_rate);\n    Color ds_color(0, 0, 0);\n\n    for (int x = 0; x &lt; width; ++x)\n    {\n      for (int y = 0; y &lt; height; ++y)\n      {\n        // for each pixel, calculate the average color of all the samples\n        ds_color.r = 0;\n        ds_color.g = 0;\n        ds_color.b = 0;\n\n        for (int i = 0; i &lt; zoom_coefficient; i++)\n        {\n          for (int j = 0; j &lt; zoom_coefficient; j++)\n          {\n            ds_color.r += sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i].r / (float)(sample_rate);\n            ds_color.g += sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i].g / (float)(sample_rate);\n            ds_color.b += sample_buffer[(y * zoom_coefficient + j) * width * zoom_coefficient + x * zoom_coefficient + i].b / (float)(sample_rate);\n          }\n        }\n\n        for (int k = 0; k &lt; 3; ++k)\n        {\n          this-&gt;rgb_framebuffer_target[3 * (y * width + x) + k] = (&amp;ds_color.r)[k] * 255;\n        }\n      }\n    }\n\n    //cout &lt;&lt; \"resolve_to_framebuffer done\" &lt;&lt; endl;\n  }\n</code></pre>"},{"location":"Homework1/Task2/#results","title":"Results","text":"<p>The following images are the result of supersampling <code>basic/test4.svg</code> with sample rates 1, 4 and 16.</p> <p>Image displayed size too small?</p> <p>Click on the image to activate lightbox mode. In lightbox mode, click on the arrows on the left and right to navigate through images. This brings easy comparison.</p> <p> </p> <p>The following images show a zoomed view of one triangle in <code>basic/test4.svg</code>. The left image is the result of sample rate 1 while the other one implements sample rate 16.</p> <p> </p> <p>In the left image, the top of the triangle seemed 'disconnected' from the other part. This is because, in the disconnected part, all the nearby sample points(center of pixels) are outside the triangle. When the sample rate increases, the subpixels inside the triangle are identified. This comparison shows the effectiveness of supersampling in antialiasing--it not only smooths the edges but also corrects the display of the image.</p>"},{"location":"Homework1/Task3/","title":"Task 3: Transforms","text":""},{"location":"Homework1/Task3/#svg-design","title":"SVG Design","text":"<p>The overall idea is to design a robot with black hair, yellow skin, grey trousers and a T-shirt with Berkeley's iconic orange and blue color. The posture of robot is standing with right arm on the waist and left arm waving.</p>"},{"location":"Homework1/Task3/#setting-up-colors","title":"Setting up colors","text":"<p>With the help of palette software, the RGB code of colors is obtained:</p> <ul> <li> Black: #000000</li> <li> Yellow: #f3cb81</li> <li> Grey: #353535</li> <li> Orange: #e09811</li> <li> Blue: #0b106e</li> </ul> <p>Filling the colors in the SVG file, the robot is rendered as follows:</p> <p></p>"},{"location":"Homework1/Task3/#transforms","title":"Transforms","text":"<p>The right arm of the robot is rotated by -45 degrees, the forearm is rotated by -90, and the relative position is slightly adjusted:(modifications are highlighted in the code below)</p> XML<pre><code>        &lt;g transform=\"translate(-90 -30)\"&gt;\n            &lt;g transform=\"rotate(-45)\"&gt;\n                &lt;g transform=\"scale(.6 .2)\"&gt;\n                    &lt;polygon fill=\"#e09811\" points=\"-50,-50 50,-50 -50,50 \" /&gt;\n                    &lt;polygon fill=\"#e09811\" points=\"-50,50 50,-50 50,50\" /&gt;\n                &lt;/g&gt;\n                &lt;g transform=\"translate(-40 50)\"&gt;\n                    &lt;g transform=\"rotate(-90)\"&gt;\n                        &lt;g transform=\"scale(.6 .2)\"&gt;\n                            &lt;polygon fill=\"#f3cb81\" points=\"-50,-50 50,-50 -50,50 \" /&gt;\n                            &lt;polygon fill=\"#f3cb81\" points=\"-50,50 50,-50 50,50\" /&gt;\n                        &lt;/g&gt;\n                    &lt;/g&gt;\n                &lt;/g&gt;\n            &lt;/g&gt;\n        &lt;/g&gt;\n</code></pre> <p>The left big arm and forearm of the robot are rotated by -45 degrees, their relative position is adjusted as well:</p> XML<pre><code>        &lt;g transform=\"translate(90 -50)\"&gt;\n            &lt;g transform=\"rotate(-45)\"&gt;\n                &lt;g transform=\"scale(.6 .2)\"&gt;\n                    &lt;polygon fill=\"#0b106e\" points=\"-50,-50 50,-50 -50,50 \" /&gt;\n                    &lt;polygon fill=\"#0b106e\" points=\"-50,50 50,-50 50,50\" /&gt;\n                &lt;/g&gt;\n                &lt;g transform=\"translate(70 -30)\"&gt;\n                    &lt;g transform=\"rotate(-45)\"&gt;\n                        &lt;g transform=\"scale(.6 .2)\"&gt;\n                            &lt;polygon fill=\"#f3cb81\" points=\"-50,-50 50,-50 -50,50 \" /&gt;\n                            &lt;polygon fill=\"#f3cb81\" points=\"-50,50 50,-50 50,50\" /&gt;\n                        &lt;/g&gt;\n                    &lt;/g&gt;\n                &lt;/g&gt;\n            &lt;/g&gt;\n        &lt;/g&gt;\n</code></pre> <p>The robot is rendered as follows:</p> <p></p>"},{"location":"Homework1/Task4/","title":"Task 4: Barycentric coordinates","text":""},{"location":"Homework1/Task4/#methodology","title":"Methodology","text":""},{"location":"Homework1/Task4/#the-barycentric-coordinate-system","title":"The Barycentric Coordinate System","text":"<p>The barycentric coordinate presents a point in a triangle with the weighted sum of the triangle's vertice positions. Assume triangle vertices \\(A(x_{A},y_{A})\\), \\(B(x_{B},y_{B})\\), and \\(C(x_{C},y_{C})\\), position of any point \\(P(x_{P},y_{P})\\) inside the triangle can be calculated as:</p> \\[ P = \\alpha A + \\beta B + \\gamma C \\] <p>where \\(\\alpha + \\beta + \\gamma = 1\\). In other words, \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are the weights showing how much each vertex contributes or is close to the point \\(P\\).</p>"},{"location":"Homework1/Task4/#calculation","title":"Calculation","text":"<p>One way to represent how close a point is to a vertex is to calculate its proportional distance to the edge opposite to the vertex, as is shown in the following figure:</p> <p></p> <p>Assume \\(D_{BC}\\) to be the distance to edge \\(BC\\), the \\(\\alpha\\) weight can be calculated as:</p> \\[ \\alpha = \\frac{D_{BC}(x_{P},y_{P})}{D_{BC}(x_{A},y_{A})} \\] <p>Point-to-line distance formula can be used to calculate \\(D\\):</p> \\[ D_{BC}(x_{P},y_{P}) = -(x_{P} - x_{B})(y_{C} - y_{B}) + (y_{P} - y_{B})(x_{C} - x_{B}) \\] \\[ D_{BC}(x_{A},y_{A}) = -(x_{A} - x_{B})(y_{C} - y_{B}) + (y_{A} - y_{B})(x_{C} - x_{B}) \\] <p>Replace \\(D_{BC}\\) in the \\(\\alpha\\) formula, we get:</p> \\[ \\alpha = \\frac{-(x_{P} - x_{B})(y_{C} - y_{B}) + (y_{P} - y_{B})(x_{C} - x_{B})}{-(x_{A} - x_{B})(y_{C} - y_{B}) + (y_{A} - y_{B})(x_{C} - x_{B})} \\] <p>Similarly, \\(\\beta\\) and \\(\\gamma\\) can be calculated as:</p> \\[ \\beta = \\frac{-(x_{P} - x_{C})(y_{A} - y_{C}) + (y_{P} - y_{C})(x_{A} - x_{C})}{-(x_{B} - x_{C})(y_{A} - y_{C}) + (y_{B} - y_{C})(x_{A} - x_{C})} \\] \\[ \\gamma = \\frac{-(x_{P} - x_{A})(y_{B} - y_{A}) + (y_{P} - y_{A})(x_{B} - x_{A})}{-(x_{C} - x_{A})(y_{B} - y_{A}) + (y_{C} - y_{A})(x_{B} - x_{A})} \\]"},{"location":"Homework1/Task4/#features-of-barycentric-coordinates","title":"Features of Barycentric Coordinates","text":"<p>The Barycentric coordinates' distinctive feature makes it suitable for geometric calculations in computer graphics. This includes but is not limited to:</p> <ul> <li>Interpolation: Barycentric coordinates can be used to interpolate values smoothly varying across a triangle. This is useful in texture mapping, color blending, and shading.</li> <li>Invariance to affine transformations: Regardless of the affine transformation applied to the triangle, the relative position of the vertices remains the same. As Barycentric coordinates are based on the relative position of the vertices, it could simplify calculation when affine transformations are involved.</li> </ul>"},{"location":"Homework1/Task4/#implementation","title":"Implementation","text":"<p>To determine whether a point is inside a triangle with Barycentric coordinates, whether the three weights are all positive is the only thing needed to check. For the points inside the triangle, its color is the weighted sum of the triangle's vertices' color, where the weights are exactly the Barycentric coordinates' weights. Other parts of the rasterization process are the same as Task 2.</p> <p>The following code shows the rasterization process with Barycentric coordinates, highlighted are the parts where Barycentric coordinates are involved:</p> C++<pre><code>    // Step 1: Convert the triangle vertices to the supersample buffer's coordinate system\n    int zoom_coefficient = sqrt(sample_rate);\n    float x0_ss = x0 * zoom_coefficient;\n    float y0_ss = y0 * zoom_coefficient;\n    float x1_ss = x1 * zoom_coefficient;\n    float y1_ss = y1 * zoom_coefficient;\n    float x2_ss = x2 * zoom_coefficient;\n    float y2_ss = y2 * zoom_coefficient;\n\n    // Step 2: Find the bounding box of the triangle in the supersample buffer\n    int bounding_box_x_min = (int)floor(min(x0_ss, min(x1_ss, x2_ss)));\n    int bounding_box_x_max = (int)ceil(max(x0_ss, max(x1_ss, x2_ss)));\n    int bounding_box_y_min = (int)floor(min(y0_ss, min(y1_ss, y2_ss)));\n    int bounding_box_y_max = (int)ceil(max(y0_ss, max(y1_ss, y2_ss)));\n\n    // Step 3: Iterate through all the pixels in the bounding box\n\n    //cout &lt;&lt; \"iterating through pixels in the bounding box\" &lt;&lt; endl;\n\n    int current_x_position, current_y_position;\n    float line01_delta_x = x1_ss - x0_ss;\n    float line01_delta_y = y1_ss - y0_ss;\n    float line12_delta_x = x2_ss - x1_ss;\n    float line12_delta_y = y2_ss - y1_ss;\n    float line20_delta_x = x0_ss - x2_ss;\n    float line20_delta_y = y0_ss - y2_ss;\n    float weight_0, weight_1, weight_2;\n    Color current_color;\n    for (current_y_position = bounding_box_y_min; current_y_position &lt;= bounding_box_y_max; current_y_position++)\n    {\n      for (current_x_position = bounding_box_x_min; current_x_position &lt;= bounding_box_x_max; current_x_position++)\n      {\n        // calcuate the weights of the barycentric coordinates\n        weight_0 = (-(current_x_position + 0.5 - x1_ss) * line12_delta_y + (current_y_position + 0.5 - y1_ss) * line12_delta_x)/(-(x0_ss - x1_ss) * line12_delta_y + (y0_ss - y1_ss) * line12_delta_x);\n        weight_1 = (-(current_x_position + 0.5 - x2_ss) * line20_delta_y + (current_y_position + 0.5 - y2_ss) * line20_delta_x)/(-(x1_ss - x2_ss) * line20_delta_y + (y1_ss - y2_ss) * line20_delta_x);\n        weight_2 = (-(current_x_position + 0.5 - x0_ss) * line01_delta_y + (current_y_position + 0.5 - y0_ss) * line01_delta_x)/(-(x2_ss - x0_ss) * line01_delta_y + (y2_ss - y0_ss) * line01_delta_x);\n\n        // check if the pixel is inside the triangle\n        if (weight_0 &gt;= 0. &amp;&amp; weight_1 &gt;= 0. &amp;&amp; weight_2 &gt;= 0.)\n        {\n          // if so, calculate the color of the pixel\n          current_color = c0 * weight_0 + c1 * weight_1 + c2 * weight_2;\n\n          // fill the corresponding buffer\n          sample_buffer[current_y_position * width * zoom_coefficient + current_x_position] = current_color;\n        }\n      }\n    }\n</code></pre>"},{"location":"Homework1/Task4/#results","title":"Results","text":"<p>The screenshot of <code>svg/basic/test7.svg</code> is shown below.</p> <p></p>"},{"location":"Homework1/Task5/","title":"Task 5: \"Pixel sampling\" for texture mapping","text":""},{"location":"Homework1/Task5/#methodology","title":"Methodology","text":""},{"location":"Homework1/Task5/#texture-coordinates-mapping","title":"Texture Coordinates Mapping","text":"<p>Given an unfolded texture image and a 3D model, the mission is to 'wrap around' the texture onto the 3D model. During this process, the 2D texture image is divided into many triangles, each of which corresponds to a triangle on the 3D model. However, the 3D model has a different coordination system from the 2D image. Therefore, texture coordinates mapping is needed to map the 2D texture image to the 3D model.</p> <p>As discussed in the previous task, the Barycentric coordinates remain the same however the triangle transforms affinely. Therefore, given a point on the 3D model, one can obtain its matching point on the 2D texture by finding out its Barycentric coordinates and then using the Barycentric coordinates to interpolate the texture coordinates. This lays the foundation for texture mapping.</p>"},{"location":"Homework1/Task5/#pixel-sampling","title":"Pixel Sampling","text":"<p>Pixel sampling finds which color should be displayed on the screen pixel. First, the corresponding point on the 3D model of the pixel sample point is found, which is the job of the rasterizer. As the color of the triangle is decided by its texture, the next step is to find the corresponding color point on the texture. After that, the color is returned all the way back to the pixel point. This whole process is described in the following figure.</p> <pre><code>flowchart TB\n  A[Start:\n  Sample Point \n  in screen space]\n  B[Rasterizer:\n  Find the corresponding point \n  on the 3D model];\n  C[Texture Mapping:\n  Find the corresponding point \n  on the texture]\n  A --&gt;|Sample Point Position| B;\n  B --&gt;|Barycentric Coordinate Position| C;\n  C --&gt;|Color| D[Rasterizer: Get color];\n  D --&gt;|Color| E[Screen frame buffer: Get color];</code></pre> <p>Featured by the density of sample points and texture texels, pixel sampling could be categorized into:</p> <ul> <li>Magnification pixel sampling: Where texels surplus sample points and each sample point corresponds to several texels. Common sample methods are nearest sampling and bilinear sampling.</li> <li>Minification pixel sampling: Where sample points surplus texels and each texel corresponds to several sample points. Common sample methods include mipmap and anisotropic filtering.</li> </ul>"},{"location":"Homework1/Task5/#magnification-pixel-sampling","title":"Magnification pixel sampling","text":""},{"location":"Homework1/Task5/#nearest-sampling","title":"Nearest sampling","text":"<p>Nearest sampling choose the closest texel to the sample point. Simple idea, low computational complexity, but nearest sampling does not ensure the color of sample pixels will change smoothly, so aliasing is common for nearest sampling. </p>"},{"location":"Homework1/Task5/#bilinear-sampling","title":"Bilinear sampling","text":"<p>Bilinear sampling finds 4 closest texels around the sample point. Both horizontal and vertical interpolations are performed to get the final color. Assume \\(u_{00}\\), \\(u_{01}\\), \\(u_{10}\\), \\(u_{11}\\) are the for texels and \\(C\\) is the color of texel, the first and second linear interpolation calculate the color of two groups of horizontal texels, based on the proportional distance horizontally sample point is close to the texels:</p> \\[ C_{0} = (1 - s) \\cdot C_{00} + s \\cdot C_{10} \\] \\[ C_{1} = (1 - s) \\cdot C_{01} + s \\cdot C_{11} \\] <p>Then, the final color is calculated by the third linear interpolation, based on the proportional distance vertically sample point is close to the texels:</p> \\[ C = (1 - t) \\cdot C_{0} + t \\cdot C_{1} \\] <p>Bilinear sampling is more complex than nearest sampling, but it can effectively reduce aliasing.</p>"},{"location":"Homework1/Task5/#implementation","title":"Implementation","text":""},{"location":"Homework1/Task5/#nearest-sampling_1","title":"Nearest sampling","text":"<p>The nearest texel of the sample point is exactly the texel sample point is in. Therefore, the position of the nearest texel is the integer part of the texture coordinates and can be easily calculated by <code>floor</code> function.</p> <p>It is worth noting that, as mipmap size varies with the level, the input texture coordinates should be scaled to the mipmap size.</p> C++<pre><code>  Color Texture::sample_nearest(Vector2D uv, int level) {\n    // TODO: Task 5: Fill this in.\n    auto&amp; mip = mipmap[level];\n\n\n    return mip.get_texel((int)floor(uv.x * mip.width), (int)floor(uv.y * mip.height));\n  }\n</code></pre>"},{"location":"Homework1/Task5/#bilinear-sampling_1","title":"Bilinear sampling","text":"<p>There is a special case for bilinear sampling: when the sample point is so close to the edge of the texture that the position of some closest texels is out of the texture. In this case, the closest texel inside texture is used to replace the out-of-bound texel. In implementation, the position of the out-of-bound texel could be replaced by the position of that closest texel. Take the horizontal direction as an example, the position of out-of-bound texel is replaced by the position of the closest texel inside the texture:</p> C++<pre><code>    //find the x coordinate of four nearest texels, as well as the proportion of the distance\n    // consider the edge case, if the sample point is on the edge, repeat the nearest texel\n    if (uv_x &lt;= 0.5)\n    {\n      sampletexel_lu[0] = 0;\n      sampletexel_ru[0] = 0;\n      sampletexel_ld[0] = 0;\n      sampletexel_rd[0] = 0;\n      // horizontally, the two nearest texels are the same\n      x_proportion = 0; // could be any value\n    }\n    else if (uv_x &gt;= width - 0.5)\n    {\n      sampletexel_lu[0] = width - 1;\n      sampletexel_ru[0] = width - 1;\n      sampletexel_ld[0] = width - 1;\n      sampletexel_rd[0] = width - 1;\n\n      x_proportion = 0;\n    }\n    else\n    {\n      sampletexel_lu[0] = floor(uv_x - 0.5);\n      sampletexel_ru[0] = ceil(uv_x - 0.5);\n      sampletexel_ld[0] = floor(uv_x - 0.5);\n      sampletexel_rd[0] = ceil(uv_x - 0.5);\n\n      x_proportion = (uv_x - 0.5) - floor(uv_x - 0.5);\n    }\n</code></pre> <p>In the first two situations of <code>if</code> statement, because horizontally the two nearest texels are the same, their weighted color sum will always be the same, which means the proportion of the distance does not need to be calculated and this is why <code>x_proportion</code> is set to 0. In other cases, <code>floor(uv_x- 0.5)</code> gives the left two texels' x coordinates and this coordinate plus by one, equals <code>ceil(</code>uv_x - 0.5)`, giving the right ones. The same logic applies to the vertical direction.</p> <p>After obtaining the position of the four nearest texels and the proportion of the distance, the color of the sample point is calculated by the formula mentioned above.</p> C++<pre><code>    //get the color of four nearest texels\n    c_lu = mip.get_texel((int)sampletexel_lu[0], (int)sampletexel_lu[1]);\n    c_ru = mip.get_texel((int)sampletexel_ru[0], (int)sampletexel_ru[1]);\n    c_ld = mip.get_texel((int)sampletexel_ld[0], (int)sampletexel_ld[1]);\n    c_rd = mip.get_texel((int)sampletexel_rd[0], (int)sampletexel_rd[1]);\n\n    //bilinear interpolation\n    // first lerp horizontally, then vertically\n    temp0 = c_lu * (1 - x_proportion) + c_ru * x_proportion;\n    temp1 = c_ld * (1 - x_proportion) + c_rd * x_proportion;\n    final = temp0 * (1 - y_proportion) + temp1 * y_proportion;\n</code></pre>"},{"location":"Homework1/Task5/#results","title":"Results","text":"<p>The following images show the result of nearest sampling and bilinear sampling. From left to right are nearest sampling with supersample rate 1, nearest sampling with supersample rate 16, bilinear sampling with supersample rate 1, and bilinear sampling with supersample rate 16.</p> <p>Image displayed size too small?</p> <p>Click on the image to activate lightbox mode. In lightbox mode, click on the arrows on the left and right to navigate through images. This brings easy comparison.</p> <p> </p> <p>Examine the images, jaggies are visible in the nearest sampling with supersampling rate 16, at the end of \"LET THERE BE\", which does not appear in bilinear sampling with supersample rate 1.</p>"},{"location":"Homework1/Task5/#discussion","title":"Discussion","text":"<p>It is clear that bilinear sampling is better than nearest sampling, even bilinear sampling with supersample rate 1 can achieve better results than nearest sampling with supersample rate 16. According to the code, bilinear sampling is certainly more costly, but it smooths the color transition, so images with many high-frequency elements would have a more obvious effect, like this twisted logo with a sharp orange-white transition. </p> <p>Another noticeable thing is bilinear sampling is more effective than nearest sampling and supersampling combined. This is because supersampling can only smooth the color transition in the pixel level, while bilinear sampling smooths the color transition in the texel level. In magnification pixel sampling, many texels are mapped to one pixel, so bilinear sampling is more effective than supersampling.</p>"},{"location":"Homework1/Task6/","title":"Task 6: \"Level sampling\" with mipmaps for texture mapping","text":""},{"location":"Homework1/Task6/#methodology","title":"Methodology","text":""},{"location":"Homework1/Task6/#texture-minification","title":"Texture Minification","text":"<p>Level sampling is a technique for solving issues in texture minification. As is described in Task 5, texture minification is the process of downsampling the texture, where multiple texels correspond to one pixel. Different ways of deciding the sample pixel's color will have a significant impact on the final image quality. </p> <p></p> <p>Take nearest sampling as an example, the color of the sample pixel is the color of the nearest texel, and all the rest corresponding texels are ignored. This could cause serious aliasing.</p> <p>As discussed in previous tasks, bilinear sampling has been proven effective in reducing aliasing. However, averaging the color of all corresponding texels is costly, as the corresponding texel number grows exponentially with the level of minification. Such difficulties are the motivation of mipmaps.</p>"},{"location":"Homework1/Task6/#mipmaps-and-level-sampling","title":"Mipmaps and Level Sampling","text":""},{"location":"Homework1/Task6/#overview","title":"Overview","text":"<p>The idea of mipmap is to store a series of downsampled textures, the downsampling process has averaged color ahead of time. When the rasterizer is doing texture mapping, it can choose the finest mipmap level in which each texel has a similar size to the sample pixel. This way, the rasterizer can avoid the expensive averaging process and still get a smooth color transition.</p> <p>The key considerations of mipmaps is how to choose the mipmap level. Once mipmap level is chosen, normal sampling process, including nearest sampling and bilinear sampling, can be applied to the chosen mipmap.</p>"},{"location":"Homework1/Task6/#mipmap-level-selection","title":"Mipmap Level Selection","text":"<p>According to the lecture, mipmap level selection can be done in several approaches:</p> <ul> <li>Fixed mipmap level: The mipmap level is chosen ahead of time, and the same level is used for all sample pixels. This is the simplest approach, and very similar to sampling without mipmaps.</li> <li>Nearest level: Based on the size of the sample pixel, the level with the closest texel size is chosen. </li> <li>Nearest level with interpolation: Select two closest levels and sample them, this will result in two selected colors. Then, the final color is interpolated between the two colors based on the size of the sample pixel.</li> </ul>"},{"location":"Homework1/Task6/#calculation-of-closest-mipmap-level","title":"Calculation of Closest Mipmap Level","text":"<p>Assuming all mipmaps have the same aspect ratio as the original texture and the sample space, mipmap level could be calculated by inspecting the distance zoom ratio between two sample pixels and their corresponding texels. Let \\(L\\) be the zoom level of distance, then the mipmap level \\(D\\) can be calculated as:</p> \\[ D = \\log_{2}L \\] <p>As each mipmap level is downsampled by a factor of 2.</p> <p>However, mipmaps do not always have the same aspect ratio as the original texture and the sample space. In this case, the three nearest pixels of the sample pixel is selected, distance zoom ratio is calculated based on the sample pixel and the x-direction nearest, and y-direction nearest respectively. The mipmap level is then calculated based on the largest distance zoom ratio, as shown in the course slides.</p>"},{"location":"Homework1/Task6/#sample-combinations","title":"Sample Combinations","text":"<p>The final color of the sample pixel is the color of the chosen mipmap level's texel. The color of the texel is calculated by normal sampling process, including nearest sampling and bilinear sampling. The mipmap level is chosen via the method discussed above. Together, there are six combinations of sample processes.</p> <p>When mipmap is sampled by bilinear sampling and the mipmap level is chosen via nearest level interpolation, the combination is called trilinear sampling.</p>"},{"location":"Homework1/Task6/#implementation","title":"Implementation","text":""},{"location":"Homework1/Task6/#the-rasterization-pipeline","title":"The rasterization pipeline","text":"<p>The rasterization pipeline is modified to include the level sampling process. The following flowchart shows the rasterization pipeline with level sampling:</p> <pre><code>flowchart TB\n  A[Rasterization Pipeline:Start]\n  B[Preprocess-Already done:\n  Triangle vertices coordinate conversion\n  Bounding box calculation];\n  C[Nearest Point Set Selection:\n  Check sample points' position\n  Find the sample points' nearest points\n  Calculate their texture coordinates];\n  D[Level Sampling:\n  Implement the sampler];\n  E[Filling Frame Buffer];\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt;|SampleParameters| D;\n  D --&gt;|Color| E;</code></pre> <p>As the most part has been implemented in previous tasks, the focus of this task is the implementation of the Nearest point set selection and level sampling.</p>"},{"location":"Homework1/Task6/#nearest-point-set-selection","title":"Nearest Point Set Selection","text":"<p>Given the sample pixel, typically its right next pixel and upper next pixel are included in the nearest point set. However, to avoid the potential risk of selecting pixels outside the sample space, when the sample pixel is on the right edge of the bounding box, the left next pixel is included instead of the right one:</p> C++<pre><code>        // find the nearest sample points\n        // the x and y coordinates of the nearest sample points\n        // in the order of current sample point, x nearest sample point, y nearest sample point\n        // consider the edge case\n        if (current_x_position == bounding_box_x_max)\n        {\n          current_nearest_x = Vector3D(current_x_position + 0.5, current_x_position - 0.5, current_x_position + 0.5);\n        }\n        else\n        {\n          current_nearest_x = Vector3D(current_x_position + 0.5, current_x_position + 1.5, current_x_position + 0.5);\n        }\n</code></pre> <p>The same applies to the y direction.</p> C++<pre><code>        if (current_y_position == bounding_box_y_max)\n        {\n          current_nearest_y = Vector3D(current_y_position + 0.5, current_y_position + 0.5, current_y_position - 0.5);\n        }\n        else\n        {\n          current_nearest_y = Vector3D(current_y_position + 0.5, current_y_position + 0.5, current_y_position + 1.5);\n        }\n</code></pre> <p>Then, the Barycentric coordinates of the three points are calculated. The first, second and third weights of the Barycentric coordinates are stored in three 3D vectors, <code>weight_0</code>, <code>weight_1</code>, <code>weight_2</code> respectively.</p> C++<pre><code>        // calcuate the weights of the barycentric coordinates\n        weight_0 = (-(current_nearest_x - Vector3D(x1_ss)) * line12_delta_y + (current_nearest_y - Vector3D(y1_ss)) * line12_delta_x)/(-(x0_ss - x1_ss) * line12_delta_y + (y0_ss - y1_ss) * line12_delta_x);\n        weight_1 = (-(current_nearest_x - Vector3D(x2_ss)) * line20_delta_y + (current_nearest_y - Vector3D(y2_ss)) * line20_delta_x)/(-(x1_ss - x2_ss) * line20_delta_y + (y1_ss - y2_ss) * line20_delta_x);\n        weight_2 = (-(current_nearest_x - Vector3D(x0_ss)) * line01_delta_y + (current_nearest_y - Vector3D(y0_ss)) * line01_delta_x)/(-(x2_ss - x0_ss) * line01_delta_y + (y2_ss - y0_ss) * line01_delta_x);\n</code></pre> <p>Next, the weight of the sample point is examined to check if the sample point is inside the triangle. If so, the texture coordinates of the nearest point set are calculated, and a new <code>SampleParams</code> object is created.</p> C++<pre><code>        // check if the pixel is inside the triangle\n        if (weight_0[0] &gt;= 0. &amp;&amp; weight_1[0] &gt;= 0. &amp;&amp; weight_2[0] &gt;= 0.)\n        {\n          // if so, calculate the the uv coordinates of the corresponding texel\n          current_uv = Vector2D(u0, v0) * weight_0[0] + Vector2D(u1, v1) * weight_1[0] + Vector2D(u2, v2) * weight_2[0];\n          // doesn't matter if dx or dy is negative, as we only need the magnitude\n          current_dx_uv = Vector2D(u0, v0) * weight_0[1] + Vector2D(u1, v1) * weight_1[1] + Vector2D(u2, v2) * weight_2[1];\n          current_dy_uv = Vector2D(u0, v0) * weight_0[2] + Vector2D(u1, v1) * weight_1[2] + Vector2D(u2, v2) * weight_2[2];\n\n          // create a SampleParams struct\n          sp.p_uv = current_uv;\n          sp.p_dx_uv = current_dx_uv;\n          sp.p_dy_uv = current_dy_uv;\n          sp.lsm = lsm;\n          sp.psm = psm;\n\n          // call the tex.sample function\n          current_color = tex.sample(sp);\n\n          // fill the corresponding buffer\n          sample_buffer[current_y_position * width * zoom_coefficient + current_x_position] = current_color;\n\n        }\n</code></pre> <p>The <code>SampleParams</code> object is then passed to the level sampling process, which will finish the sample process and return the color of the sample pixel. The final step is to fill the corresponding buffer with the returned color.</p>"},{"location":"Homework1/Task6/#sampler","title":"Sampler","text":"<p>The sampler requires a continuous mipmap level value to implement nearest level and interpolation sampling. This is done by <code>get_level</code> function.</p> <p>The calculation process follows the method discussed in the methodology section. Noting that the texture coordinates should be scaled to the texture size.</p> <p>The calculated level is only valid in the range of the mipmap levels. In the case of texture minification, the calculated level could be negative and need to be clamped to 0. In extreme cases, the calculated level could be larger than the number of mipmap levels and need to be clamped to the largest level.</p> <p>To avoid calculating the largest level every time, the largest level is stored in the <code>Texture</code> class as a member variable.</p> C++<pre><code>struct Texture {\n  size_t width;\n  size_t height;\n  std::vector&lt;MipLevel&gt; mipmap;\n  int numSubLevels;\n\n  void init(const vector&lt;unsigned char&gt;&amp; pixels, const size_t&amp; w, const size_t&amp; h) {\n    width = w; height = h;\n\n    // A fancy C++11 feature. emplace_back constructs the element in place,\n    // and in this case it uses the new {} list constructor syntax.\n    mipmap.emplace_back(MipLevel{width, height, pixels});\n\n    generate_mips();\n\n    numSubLevels = (int)(log2f((float)max(width, height)));\n  }\n}\n</code></pre> <p>Combining all these considerations, the <code>get_level</code> function is implemented as follows:</p> C++<pre><code>  float Texture::get_level(const SampleParams&amp; sp) {\n    // TODO: Task 6: Fill this in.\n    // calculate the level of the mipmap to sample from\n    float Lx, Ly, L, D;\n    Lx = sqrt(pow((sp.p_dx_uv.x - sp.p_uv.x) * width, 2) + pow((sp.p_dx_uv.y - sp.p_uv.y) * height, 2));\n    Ly = sqrt(pow((sp.p_dy_uv.x - sp.p_uv.x) * width, 2) + pow((sp.p_dy_uv.y - sp.p_uv.y) * height, 2));\n    L = max(Lx, Ly);\n\n    D = log2(L);\n\n    // in texture magnification, many pixels are mapped to one texel, so the level should be 0\n    if (D &lt; 0)\n    {\n      D = 0;\n    }\n\n    // in extreme texture minification, one texel is mapped to many pixels, so the level should be the maximum level\n\n    if (D &gt; numSubLevels - 1)\n    {\n      D = numSubLevels - 1;\n    }\n    return D;\n  }\n</code></pre> <p>The rest part of the sampler is straightforward, which uses several <code>if</code> statements to cover all sample combinations.</p> <p>Always sample at 0 level: C++<pre><code>    if (sp.lsm == L_ZERO)\n    {\n      // sample from mipmap level 0\n      if (sp.psm == P_NEAREST)\n      {\n        return sample_nearest(sp.p_uv, 0);\n      }\n      else if (sp.psm == P_LINEAR)\n      {\n        return sample_bilinear(sp.p_uv, 0);\n      }\n      else\n      {\n        return Color(1, 0, 1);\n      }\n    }\n</code></pre></p> <p>Nearest level sampling: C++<pre><code>    else if (sp.lsm == L_NEAREST)\n    {\n      // sample from the nearest mipmap level\n      int level = (int)round(get_level(sp));\n\n      if (sp.psm == P_NEAREST)\n      {\n        return sample_nearest(sp.p_uv, level);\n      }\n      else if (sp.psm == P_LINEAR)\n      {\n        return sample_bilinear(sp.p_uv, level);\n      }\n      else\n      {\n        return Color(1, 0, 1);\n      }\n    }\n</code></pre></p> <p>Nearest level with interpolation sampling: C++<pre><code>    else if (sp.lsm == L_LINEAR)\n    {\n      // sample from the two nearest mipmap levels and linearly interpolate\n      Color c0, c1, result;\n      if (sp.psm == P_NEAREST)\n      {\n        c0 = sample_nearest(sp.p_uv, floor(get_level(sp)));\n        c1 = sample_nearest(sp.p_uv, ceil(get_level(sp)));\n      }\n      else if (sp.psm == P_LINEAR)\n      {\n        c0 = sample_bilinear(sp.p_uv, floor(get_level(sp)));\n        c1 = sample_bilinear(sp.p_uv, ceil(get_level(sp)));\n      }\n      else\n      {\n        result = Color(1, 0, 1);\n      }\n      result = c0 * (1 - get_level(sp) + floor(get_level(sp))) + c1 * (get_level(sp) - floor(get_level(sp)));\n      return result;\n    }\n</code></pre> Where <code>c0</code> and <code>c1</code> are the colors of the two nearest mipmap levels, and <code>result</code> is the final color of the sample pixel. Two nearest levels are determined by the <code>floor</code> and <code>ceil</code> function.</p>"},{"location":"Homework1/Task6/#results","title":"Results","text":"<p>A carefully selected example is used to demonstrate the effect of level sampling. The original texture is a 3840 * 2160 resolution screenshot of the computer game Atomic Heart.</p> <p>SVG <code>svg/texmap/test6.svg</code>is modified and stored as <code>docs/test6.svg</code> to apply the texture.</p> <p>To recode the rasterizing time of SVG, <code>clock</code> function is applied to the draw end. The rasterization time of each sampling combination is recorded.</p> <p>The following figures show the rasterization option of (level 0 sampling, nearest sampling, sample rate 1) and (level 0 sampling, nearest sampling, sample rate 16). The left figure is the simplest case without any particular sampling technique, and the other one is used as an example of no aliasing.</p> <p> </p> <p>While the left one is aliased, it only takes <code>0.013008</code> seconds to rasterize. The right one is much smoother, but it takes <code>0.07369</code> seconds to rasterize. The rasterization time is increased by 5.66 times.</p> <p>The following figures shows the effect of level sampling. The left figure has option (nearest level sampling, nearest sampling, sample rate 1) and the right one has option (nearest level with interpolation sampling, nearest sampling, sample rate 1). The rasterization time of the two is <code>0.007655</code> and <code>0.017561</code> seconds respectively.</p> <p> </p> <p>With nearest-level sampling, as the mipmap texture size is reduced, the rasterization time is even less than the original texture. While nearest-level sampling removed aliasing, because the texture level does not match precisely with the sample pixel's level, the text is blurred. By contrast, nearest level with interpolation sampling has a smoother result, but the rasterization time is increased by 2.3 times, which achieves a balance between the quality and the speed.</p> <p>If level sampling technique is combined with bilinear sampling, the result is even better. The following figures show the comparison of trilinear sampling(left) and supersampling with sample rate 16(right).</p> <p> </p> <p>The rasterization time of the two is <code>0.031648</code> and <code>0.07369</code> seconds respectively. The trilinear takes only half of the time of super sampling, while the result is similar.</p>"},{"location":"Homework1/Task6/#conclusion","title":"Conclusion","text":"<p>Level sampling is introduced to solve the aliasing problem in texture magnification and minification without the need for expensive supersampling. </p> <p>Nearest-level sampling is one simple approach, because of the minification of the texture, the rasterization time is even less than the original texture, as well as the memory consumption. It can remove aliasing, but result might be blurred as the texture level does not match precisely with the sample pixel's level.</p> <p>Nearest level with interpolation sampling has a smoother result, but loading two nearest levels and linearly interpolating color requires more time and memory. Based on the test, it achieves a balance between the quality and the speed.</p> <p>Level sampling techniques can be combined with bilinear sampling to achieve even better results. For example, in our test, trilinear sampling achieves the best result along all sample combinations except for supersampling. While the rasterization time is also the second longest, it is trivial when compared to the time of supersampling, and the memory consumption is close to nearest level with interpolation sampling.</p>"},{"location":"Homework2/Overview/","title":"Homework2: Meshedit Overview","text":""},{"location":"Homework2/Overview/#overview","title":"Overview","text":"<p>From this homework, we learned and practiced the following concepts: - Halfedge Mesh: We implemented a halfedge mesh data structure and various operations on it. - MeshEdit: We implemented various mesh editing operations, such as edge flip, edge split, and vertex split. - Bezier Curve and Patch: We implemented the de Casteljau's algorithm to evaluate points on a B\u00e9zier curve and patch.</p> <p>Besides key concepts, throughout the homework, we also learned about the following: - Debugging large codebase: We learned how to debug a large, unfamiliar codebase. - Code organization: We learned how to make code more readable and maintainable.</p>"},{"location":"Homework2/Overview/#updates-to-the-website","title":"Updates to the website","text":"<p>We implemented the following updates to the website: - Added <code>.csv</code> table support - Added print webpage button for each page</p> Note for PDF <p>Some features of the website, such as picture lightbox, may not be available in the PDF version. We recommend reading the website directly for the best experience.</p>"},{"location":"Homework2/Section1/Part1/","title":"Part 1: Bezier Curves with 1D de Casteljau Subdivision","text":""},{"location":"Homework2/Section1/Part1/#methodology","title":"Methodology","text":"<p>de Casteljau's algorithm is a recursive algorithm that can be used to evaluate a Bezier curve. A Bezier curve is defined by a set of control points, and the curve itself is a linear combination of these control points. Assuming \\(t\\) to be the proportion of the curve that has been traversed, this algorithm can find the corresponding point's position \\(B(t)\\) from the curve.</p> <p>The de Casteljau's algorithm can be described as follows:</p> <ol> <li> <p>Given a set of control points \\(P_0, P_1, \\dots, P_n\\), an empty intermediate point set \\(Q\\) and a parameter \\(t\\). Set \\(Q = P\\).</p> </li> <li> <p>Check if the length of \\(Q\\) is 1. If so, return the only point in \\(Q\\).</p> </li> <li> <p>Otherwise, for each pair of points \\(Q_i, Q_{i+1}\\) in \\(Q\\), calculate the point by linear interpolating:</p> </li> </ol> \\[ Q'_i = (1-t)Q_i + tQ_{i+1}. \\] <ol> <li>Set \\(Q = Q'\\) and repeat from step 2.</li> </ol> <p>Among each loop, the number of points in \\(Q\\) will decrease by 1, and the last point in \\(Q\\) will be the point on the Bezier curve. To draw a Bezier curve, we can progressively evaluate the curve at different \\(t\\) values and connect the points together.</p>"},{"location":"Homework2/Section1/Part1/#implementation","title":"Implementation","text":"<p>In this task, we are required to implement the interpolate function of de Casteljau's algorithm. The function takes a list of intermediate points and a parameter <code>t</code>, and returns the new intermediate points after interpolation.</p> <p>The implementation idea is simple: for each pair of points in the input <code>points</code> vector, calculate the intermediate point and store it in the <code>result</code> vector: C++<pre><code>  std::vector&lt;Vector2D&gt; BezierCurve::evaluateStep(std::vector&lt;Vector2D&gt; const &amp;points)\n  {\n    // TODO Part 1.\n    // Implement by Ruhao Tian starts here\n\n    // Create a vector iterator\n    std::vector&lt;Vector2D&gt;::const_iterator it = points.begin();\n\n    // Create a empty vector to store the result\n    std::vector&lt;Vector2D&gt; result;\n\n    // Loop through the points and calculate the intermediate points\n    // iterater should stop at the second last point\n    while (it != points.end() - 1)\n    {\n      // Calculate the intermediate point\n      Vector2D intermediate = (1 - t) * (*it) + t * (*(it + 1));\n      // Add the intermediate point to the result vector\n      result.push_back(intermediate);\n      // Move the iterator to the next point\n      it++;\n    }\n\n    return result;\n  }\n</code></pre></p>"},{"location":"Homework2/Section1/Part1/#results","title":"Results","text":"<p>A 6-point Bezier curve is created to test the implementation:</p> Text Only<pre><code>6\n0.200 0.350   0.300 0.600   0.500 0.750   0.700 0.450  1.000 0.900   0.800 0.800\n</code></pre> <p>The figures below show the drawn curve as well as levels of evaluation. The left one is the curve without moving control points, and the right one is the curve with control points manually moved.</p> <p> </p>"},{"location":"Homework2/Section1/Part2/","title":"Part2: Bezier Surfaces with Separable 1D de Casteljau","text":""},{"location":"Homework2/Section1/Part2/#methodology","title":"Methodology","text":"<p>The main way to use de Casteljau's algorithm on the surface is:  First, split the two-dimensional control point matrix into n one-dimensional matrices. Then, apply a method similar to Part 1 to each of these n one-dimensional matrices, obtaining coordinates for n points. Finally, use these n points coordinates for one more calculation to get the target point.</p> <p>The detailed steps are listed as follows:</p> <ol> <li> <p>Given a two-dimensional control point matrix P (n rows and m columns). Split the two-dimensional control point matrix into n rows.</p> </li> <li> <p>Apply the de Casteljau's algorithm separately to these n columns, and iterate m-1 times to obtain n final points. This operation is similar to Part 1, where n control points and parameter \\(t\\) are used to obtain points on the B\u00e9zier curve, except here the parameter is \\(u\\).</p> </li> <li> <p>Use these n points to perform the same operation(parameter is \\(v\\)), obtaining a point that lies on the B\u00e9zier patch.</p> </li> </ol> <p>Summary: A total of \\(n * (m-1) + 1\\) iterations are required. There are n columns in total, each column requires m-1 iterations. Finally, the n points are iterated once more.</p>"},{"location":"Homework2/Section1/Part2/#implementation","title":"Implementation","text":"<p>The task requires returning a point that lies on the B\u00e9zier surface.</p> <p>First, evaluates one step of de Casteljau's algorithm using the given points and the scalar parameter t. C++<pre><code>  std::vector&lt;Vector3D&gt; BezierPatch::evaluateStep(std::vector&lt;Vector3D&gt; const &amp;points, double t) const\n  {\n    // TODO Part 2.\n    std::vector&lt;Vector3D&gt;newPoints;\n\n    double x,y,z;\n    for(int i=0;i&lt;points.size()-1;i++){\n      x=points[i].x*t+points[i+1].x*(1-t);\n      y=points[i].y*t+points[i+1].y*(1-t);\n      z=points[i].z*t+points[i+1].z*(1-t);\n      newPoints.emplace_back(x,y,z);\n    }\n    return newPoints;\n  }\n</code></pre></p> <p>Second, fully evaluates de Casteljau's algorithm for a vector of points at scalar parameter t. C++<pre><code>  Vector3D BezierPatch::evaluate1D(std::vector&lt;Vector3D&gt; const &amp;points, double t) const\n  {\n    // TODO Part 2.\n\n    //Below is for iteration. If its size==1, it is the goal point.\n    std::vector&lt;Vector3D&gt;operatePoints=points;\n\n    double x,y,z;\n    while(operatePoints.size()&gt;1){\n      operatePoints=evaluateStep(operatePoints,t);\n    }\n    return operatePoints.front();\n  }\n</code></pre></p> <p>Finally, evaluate the Bezier patch at parameter (u, v) C++<pre><code>Vector3D BezierPatch::evaluate(double u, double v) const \n  {  \n    // TODO Part 2.\n\n    //Fully evaluate de Casteljau's algorithm for each row of the \n    //two-dimensional control. Result is rowPoints.\n    std::vector&lt;Vector3D&gt; rowPoints;\n    for(int i=0;i&lt;controlPoints.size();i++){\n      rowPoints.push_back(evaluate1D(controlPoints[i],u));\n    }\n\n    //Fully evaluate de Casteljau's algorithm for rowPoints\n    //Result is patchPoint(the goal)\n    Vector3D patchPoint;\n    patchPoint=evaluate1D(rowPoints,v);\n    return patchPoint;\n  }\n</code></pre></p>"},{"location":"Homework2/Section1/Part2/#results","title":"Results","text":"<p>The figures below show the drawn picture of the teapot.bez</p> <p> </p>"},{"location":"Homework2/Section2/Part3/","title":"Part 3: Area-Weighted Vertex Normals","text":""},{"location":"Homework2/Section2/Part3/#methodology","title":"Methodology","text":"<p>Given a vertex on the mesh, the normal vector of the vertex is the average of the normal vectors of all the triangles that share the vertex. Assume \\(N_0, N_1, \\dots, N_k\\) are the normal vectors of the triangles that share the vertex, the normal vector of the vertex is calculated as follows:</p> \\[ N = \\frac{N_0 * A_0 + N_1 * A_1 + \\dots + N_k * A_k}{||N_0 * A_0 + N_1 * A_1 + \\dots + N_k * A_k||} \\] <p>where \\(A_0, A_1, \\dots, A_k\\) are the areas of the triangles that share the vertex. The normal vector is normalized to ensure it has a unit length.</p>"},{"location":"Homework2/Section2/Part3/#implementation","title":"Implementation","text":"<p>The general idea is to iterate through all faces and calculate the normal vector and area of each face. In halfedge data structure, each face has a corresponding halfedge, and visiting the next face counter-clockwise is equal to visiting its half edge:</p> C++<pre><code>next_halfedge = current_halfedge-&gt;twin()-&gt;next();\n</code></pre> <p>And when <code>current_halfedge == start_halfedge</code>, the iteration is finished.</p> <p>For each face, the normal vector can be obtained by the provided <code>normal</code> function: C++<pre><code>current_face_normal = current_halfedge-&gt;face()-&gt;normal();\n</code></pre></p> <p>To calculate the area of the face, first, the three vertices of the face are obtained: C++<pre><code>      vertex_position[0] = current_halfedge-&gt;vertex()-&gt;position;\n      vertex_position[1] = current_halfedge-&gt;next()-&gt;vertex()-&gt;position;\n      vertex_position[2] = current_halfedge-&gt;next()-&gt;next()-&gt;vertex()-&gt;position;\n</code></pre></p> <p>Then the area of the face is calculated by the cross product of two edges of the face: C++<pre><code>    current_face_area = cross(vertex_position[1] - vertex_position[0], vertex_position[2] - vertex_position[0]).norm() / 2;\n</code></pre></p> <p>During the iteration, the normal vector and area of each face are accumulated to an intermediate normal vector, which will be normalized and returned when the iteration is finished: C++<pre><code>      intermediate_normal += current_face_normal * current_triangle_area;\n</code></pre></p> <p>Putting all the pieces together, the implementation of the <code>normal</code> function is as follows: C++<pre><code>  Vector3D Vertex::normal( void ) const\n  {\n    // TODO Part 3.\n    // Returns an approximate unit normal at this vertex, computed by\n    // taking the area-weighted average of the normals of neighboring\n    // triangles, then normalizing.\n\n    // Implementaion by Ruhao Tian starts here\n\n    // Prepare intermediate variables\n    HalfedgeCIter current_halfedge, start_halfedge, next_halfedge;\n    Vector3D intermediate_normal,current_face_normal;\n    double current_triangle_area;\n    Vector3D vertex_position[3];\n\n    // Initialize the normal vector\n    intermediate_normal = Vector3D(0, 0, 0);\n    current_halfedge = start_halfedge = this-&gt;halfedge();\n\n    // Loop through the halfedges of the vertex\n    do {\n      // for safety, first obtain the next halfedge, although this is not mandatory\n      next_halfedge = current_halfedge-&gt;twin()-&gt;next();\n\n      // obtain the face normal\n      current_face_normal = current_halfedge-&gt;face()-&gt;normal();\n\n      // calculate the area of the triangle\n      // first, obtain the three vertices position\n      vertex_position[0] = current_halfedge-&gt;vertex()-&gt;position;\n      vertex_position[1] = current_halfedge-&gt;next()-&gt;vertex()-&gt;position;\n      vertex_position[2] = current_halfedge-&gt;next()-&gt;next()-&gt;vertex()-&gt;position;\n      // use the cross product to calculate the area\n      current_triangle_area = cross(vertex_position[1] - vertex_position[0], vertex_position[2] - vertex_position[0]).norm() / 2;\n\n      // add the area-weighted normal to the intermediate normal\n      intermediate_normal += current_face_normal * current_triangle_area;\n\n      // move to the next halfedge\n      current_halfedge = next_halfedge;\n    } while (current_halfedge != start_halfedge);\n\n    // normalize the intermediate normal\n    intermediate_normal.normalize();\n    return intermediate_normal;\n  }\n</code></pre></p>"},{"location":"Homework2/Section2/Part3/#results","title":"Results","text":"<p>The left figure below is the screenshot of <code>dae/teapot.dae</code> without normal vector shading and the right one is the same mesh with normal vector shading enabled. Compared with the left one, figure implemented normal vector clearly has smoother shading.</p> <p>Image displayed size too small?</p> <p>Click on the image to activate lightbox mode. In lightbox mode, click on the arrows on the left and right to navigate through images. This brings easy comparison.</p> <p> </p>"},{"location":"Homework2/Section2/Part4/","title":"Part2: Bezier Surfaces with Separable 1D de Casteljau","text":""},{"location":"Homework2/Section2/Part4/#methodology","title":"Methodology","text":"<p>Get the flip image by modifying the given halfedge and related parameter pointers. Below is an example of modifications made to a halfedge:</p> <p>As shown in the diagram, we now need to modify the pointers related to the halfedge h.</p> <p></p> <p>After modification: 1. The starting point of the halfedge becomes A, and it points to D. 2. The twin() of the halfedge remains unchanged, but it now points to the modified halfedge in the opposite direction. 3. The next() of the halfedge becomes CB. 4. The face which the halfedge points to becomes ABC.</p> <p>Similar operations will also be performed on vertices, faces, and so on. This includes but is not limited to, the following changes: The half-edge pointed to by A becomes h. The faces change from ABD, BCD to ABC, ACD.</p> <p>The rest of the operations are very similar, so I won't go into further detail. But you can see any information in code.</p>"},{"location":"Homework2/Section2/Part4/#implementation","title":"Implementation","text":"<p>Before providing my code, I would like to explain the parameters:</p> <ol> <li>h is the h shown in the diagram.</li> <li>th=h-&gt;twin()</li> <li>h/th_next=h/th-&gt;next()</li> <li>h/th_pre-&gt;next()=h/th C++<pre><code>  HalfedgeIter h=e0-&gt;halfedge();\n  HalfedgeIter th=h-&gt;twin();\n  HalfedgeIter h_next=h-&gt;next();\n  HalfedgeIter th_next=th-&gt;next();\n  HalfedgeIter h_pre=h_next-&gt;next();\n  HalfedgeIter th_pre=th_next-&gt;next();\n</code></pre></li> </ol> <p>v1,v2,v3,v4 is A,B,C,D C++<pre><code>  VertexIter v1=h_pre-&gt;vertex();\n  VertexIter v2=h-&gt;vertex();\n  VertexIter v4=th_pre-&gt;vertex();\n  VertexIter v3=th-&gt;vertex();\n</code></pre></p> <p>Total solution code: C++<pre><code>EdgeIter HalfedgeMesh::flipEdge( EdgeIter e0 )\n  {\n    // TODO Part 4.\n    // This method should flip the given edge and return an iterator to the flipped edge.\n\n    //edge is not allowed\n    if(e0-&gt;isBoundary()) return e0;\n\n\n    HalfedgeIter h=e0-&gt;halfedge();\n    HalfedgeIter th=h-&gt;twin();\n    HalfedgeIter h_next=h-&gt;next();\n    HalfedgeIter th_next=th-&gt;next();\n    HalfedgeIter h_pre=h_next-&gt;next();\n    HalfedgeIter th_pre=th_next-&gt;next();\n\n    VertexIter v1=h_pre-&gt;vertex();\n    VertexIter v2=h-&gt;vertex();\n    VertexIter v4=th_pre-&gt;vertex();\n    VertexIter v3=th-&gt;vertex();\n\n    FaceIter f1=h-&gt;face();\n    FaceIter f2=th-&gt;face();\n\n    //change the halfedge pointer\n    h-&gt;setNeighbors(th_pre,th,v1,h-&gt;edge(),f1);\n    th_pre-&gt;setNeighbors(h_next,th_pre-&gt;twin(),v4,th_pre-&gt;edge(),f1);\n    h_next-&gt;setNeighbors(h,h_next-&gt;twin(),v3,h_next-&gt;edge(),f1);\n    th-&gt;setNeighbors(h_pre,h,v4,h-&gt;edge(),f2);\n    h_pre-&gt;setNeighbors(th_next,h_pre-&gt;twin(),v1,h_pre-&gt;edge(),f2);\n    th_next-&gt;setNeighbors(th,th_next-&gt;twin(),v2,th_next-&gt;edge(),f2);\n\n    //change the vertex pointer\n    v1-&gt;halfedge()=h;\n    v2-&gt;halfedge()=th_next;\n    v3-&gt;halfedge()=h_next;\n    v4-&gt;halfedge()=th;\n\n\n    //change the face pointer\n    f1-&gt;halfedge()=h;\n    f2-&gt;halfedge()=th;\n\n    return e0;\n  }\n</code></pre></p>"},{"location":"Homework2/Section2/Part4/#results","title":"Results","text":"<p>The figure below shows the original teapot.</p> <p> </p> <p>The following image shows the teapot after flipping 4 edges. I have highlighted the modifications with a yellow highlighter.</p> <p> </p>"},{"location":"Homework2/Section2/Part4/#debug-experience","title":"Debug experience","text":"<p>I used to make a very simple but fatal problem, which I used HalfedgeCIter instead of HalfedgeIter. So, I found the program always occurred crashed. Finally, with the assistance of my partner and the ChatGPT, I noticed and understood the difference between CIter and Iter.</p>"},{"location":"Homework2/Section2/Part5/","title":"Part 5: Edge Split","text":""},{"location":"Homework2/Section2/Part5/#methodology","title":"Methodology","text":"<p>The method of edge splitting is similar to Part 4, but it will be a bit more complex. This requires us to consider a new vertex and the 6 new halfedges that are created. We adjust their values and pointers to accomplish the edge split. Here is a simple example explaining the adjustment of pointers for the halfedge h and the new vertex E.</p> <p> </p> <p>for h after modification:</p> <ol> <li>The starting point of the halfedge remains B, but it points to E now.</li> <li>The twin() becomes the EB.</li> <li>The next() of the halfedge becomes EA.</li> <li>The face that the halfedge points to becomes ABE.</li> </ol> <p>for e after modification:</p> <ol> <li>Its position is descripted by \\((E_x,E_y)=0.5*[(B_x,B_y)+(D_x,D_y)]\\)</li> <li>It points to ED.</li> </ol> <p>Similar operations will also be performed on vertices, faces, and so on. This includes, but is not limited to, the following changes:</p> <ul> <li>The half-edge pointed to by A becomes AE</li> <li>Now there are four faces (ABE, AED, DEC, BEC)</li> </ul> <p>The rest of the operations are similar.</p>"},{"location":"Homework2/Section2/Part5/#implementation","title":"Implementation","text":"<p>Notations of the implementation are shown in the following figure:</p> <p></p> <p>To assist coding, data structures and relevant values before and after the edge split are listed in the following tables.</p> <p>For the halfedges:</p> name next twin vertex edge face j k j_twin a ab f1 k l o b bc f1 l j l_twin c ca f1 m n m_twin b bd f2 n o n_twin d dc f2 o m k c bc f2 j\u2018 x j_twin a ab f3 k\u2019 l o e ec(bc) f1 l\u2018 p l_twin c ca f1 m\u2019 s m_twin b bd f4 n\u2018 o n_twin d dc f2 o\u2019 r k c ec(bc) f2 p\u2018 k q a ae f1 q\u2019 j p e ae f3 r\u2018 n s e ed f2 s\u2019 y r d ed f4 x\u2018 q y b be f3 y\u2019 m x e be f4 <p>Where the new halfedges are marked with single quotes, the same applies to the vertices, edges and faces.</p> <p>For the vertices:</p> name a b c d a' b' c' d' e' halfedge j k/m o/l n p/j x/m o/l s/n k <p>For the edges:</p> name ab bc ca bd dc ab' be' ea' ec'(bc) ca' bd' dc' de' halfedge j k/o l m n j x/y q/p k l m n r/s <p>For the faces:</p> name f1 f2 f1' f2' f3' f4' halfedge j/k/l m/n/o l/p/k o/r/n j/x/q m/s/y <p>With help of the above tables, the implementation is simple as follows: C++<pre><code>VertexIter HalfedgeMesh::splitEdge( EdgeIter e0 )\n  {\n    // TODO Part 5.\n    // This method should split the given edge and return an iterator to the newly inserted vertex.\n    // The halfedge of this vertex should point along the edge that was split, rather than the new edges.\n    HalfedgeIter j, k, l, m, n, o, j_new, k_new, l_new, m_new, n_new, o_new, p_new, q_new, r_new, s_new,x_new,y_new;\n    k_new = k = e0-&gt;halfedge();\n    l_new = l = k-&gt;next();\n    j_new = j = l-&gt;next();\n    o_new = o = e0-&gt;halfedge()-&gt;twin();\n    m_new = m = o-&gt;next();\n    n_new = n = m-&gt;next();\n    p_new = newHalfedge();\n    q_new = newHalfedge();\n    r_new = newHalfedge();\n    s_new = newHalfedge();\n    x_new = newHalfedge();\n    y_new = newHalfedge();\n\n    VertexIter a, b, c, d, a_new, b_new, c_new, d_new, e_new;\n    a_new = a = j-&gt;vertex();\n    b_new = b = k-&gt;vertex();\n    c_new = c = l-&gt;vertex();\n    d_new = d = n-&gt;vertex();\n    e_new = newVertex();\n\n    EdgeIter ab, bc, ca, bd, dc, ab_new, bc_new_ec, ca_new, bd_new, dc_new, ae_new, be_new, ed_new;\n    ab_new = ab = j-&gt;edge();\n    bc_new_ec = bc = k-&gt;edge();\n    ca_new = ca = l-&gt;edge();\n    bd_new = bd = m-&gt;edge();\n    dc_new = dc = n-&gt;edge();\n    ae_new = newEdge();\n    be_new = newEdge();\n    ed_new = newEdge();\n\n    FaceIter f1, f2, f1_new, f2_new, f3_new, f4_new;\n    f1_new = f1 = k-&gt;face();\n    f2_new = f2 = o-&gt;face();\n    f3_new = newFace();\n    f4_new = newFace();\n\n    // Update the halfedges\n    j_new-&gt;setNeighbors(x_new, j-&gt;twin(), a, ab, f3_new);\n    k_new-&gt;setNeighbors(l,o,e_new,bc,f1);\n    l_new-&gt;setNeighbors(p_new,l-&gt;twin(),c,ca,f1);\n    m_new-&gt;setNeighbors(s_new,m-&gt;twin(),b,bd,f4_new);\n    n_new-&gt;setNeighbors(o,n-&gt;twin(),d,dc,f2);\n    o_new-&gt;setNeighbors(r_new,k,c,bc,f2);\n    p_new-&gt;setNeighbors(k,q_new,a,ae_new,f1);\n    q_new-&gt;setNeighbors(j,p_new,e_new,ae_new,f3_new);\n    r_new-&gt;setNeighbors(n_new,s_new,e_new,ed_new,f2);\n    s_new-&gt;setNeighbors(y_new,r_new,d,ed_new,f4_new);\n    x_new-&gt;setNeighbors(q_new,y_new,b,be_new,f3_new);\n    y_new-&gt;setNeighbors(m,x_new,e_new,be_new,f4_new);\n\n    // Update the vertices\n    a_new-&gt;halfedge() = j;\n    b_new-&gt;halfedge() = m;\n    c_new-&gt;halfedge() = l;\n    d_new-&gt;halfedge() = n;\n    e_new-&gt;halfedge() = k;\n\n    // Update the edges\n    ab_new-&gt;halfedge() = j;\n    be_new-&gt;halfedge() = y_new;\n    ae_new-&gt;halfedge() = q_new;\n    bc_new_ec-&gt;halfedge() = k;\n    ca_new-&gt;halfedge() = l;\n    bd_new-&gt;halfedge() = m;\n    dc_new-&gt;halfedge() = n;\n    ed_new-&gt;halfedge() = s_new;\n\n    // Update the faces\n    f1_new-&gt;halfedge() = k;\n    f2_new-&gt;halfedge() = o;\n    f3_new-&gt;halfedge() = j;\n    f4_new-&gt;halfedge() = m;\n\n    // Update the position of the new vertex\n    // The new vertex should be the average of the two original vertices, b and c\n    e_new-&gt;position = (b-&gt;position + c-&gt;position) / 2;\n    return e_new;\n  }\n</code></pre></p> <p>With careful consideration of the pointers and values, the implementation is one-shot correct and has undergone no debugging process.</p>"},{"location":"Homework2/Section2/Part5/#results","title":"Results","text":"<p>The figure below is the original teapot.</p> <p></p> <p>After some edge splits:</p> <p></p> <p>After a combination of both edge splits and edge flips:</p> <p></p>"},{"location":"Homework2/Section2/Part6/","title":"Part 6: Loop Subdivision for Mesh Upsampling","text":""},{"location":"Homework2/Section2/Part6/#methodology","title":"Methodology","text":"<p>Loop subdivision is a mesh upsampling technique for triangle meshes. It is a simple and efficient way to increase the number of triangles in a mesh. The algorithm involves two main steps: triangle subdivision and vertex position update.</p>"},{"location":"Homework2/Section2/Part6/#triangle-subdivision","title":"Triangle Subdivision","text":"<p>The subdivision of a triangle is done by adding a new vertex at the center of each edge and connecting the new vertices to form 4 new triangles. </p> <p></p> <p>Triangle subdivision can be decomposed into two steps:</p> <ol> <li>Split all edges of the mesh</li> <li>For the new edges, flip the one that connects both new and old vertices</li> </ol>"},{"location":"Homework2/Section2/Part6/#vertex-position-update","title":"Vertex Position Update","text":"<p>The position of vertices is determined by the surrounding vertices.</p> <p></p> <p>For a new vertex splitting edge \\(AB\\), the new position is calculated as:</p> \\[ \\frac{3}{8}(A + B) + \\frac{1}{8}(C + D) \\] <p>where \\(C\\) and \\(D\\) are the vertices of the adjacent triangles to the edge \\(AB\\).</p> <p>For an existing vertex, the new position is updated as:</p> \\[ (1-n*u)*P + {u} * \\sum_{i=1}^{n}P_i \\] <p>where \\(P\\) is the original position of the vertex, \\(P_i\\) is the position of the \\(i^{th}\\) vertex connected to \\(P\\), and \\(n\\) is degree of \\(P\\). \\(u\\) is a constant that depends on \\(n\\), as described in the figure.</p>"},{"location":"Homework2/Section2/Part6/#implementation","title":"Implementation","text":""},{"location":"Homework2/Section2/Part6/#subdivision-overview","title":"Subdivision Overview","text":"<p>The recommended steps provided by the course website are followed to implement the Loop subdivision algorithm, as illustrated below:</p> <pre><code>flowchart TB\n  A[Compute new positions for all the vertices in the input mesh]\n  B[Compute the updated vertex positions associated with edges];\n  C[Split every edge in the mesh];\n  D[Flip any new edge that connects an old and new vertex];\n  E[Copy the new vertex positions into final Vertex::position];\n  A --&gt; B;\n  B --&gt; C;\n  C --&gt; D;\n  D --&gt; E;</code></pre> <p>Each step is explained in detail in the following sections.</p>"},{"location":"Homework2/Section2/Part6/#compute-existing-vertex-positions","title":"Compute Existing Vertex Positions","text":"<p>Given a vertex in the mesh, the calculation of its updated position involves obtaining all the surrounding vertices and computing the new position based on the degree of the vertex. </p> <p>To iterate over all connected vertices, halfedge iterator <code>current_halfedge</code> and <code>start_halfedge</code> are used to control the traversal. The degree <code>n</code> is incremented for each visited vertex, and the position of the vertex is added to the sum <code>new_position</code>.</p> C++<pre><code>      // initialize the new position\n      new_position = Vector3D(0, 0, 0);\n      current_halfedge = start_halfedge = v-&gt;halfedge();\n      n = 0;\n\n      // loop through the halfedges of the vertex\n      do\n      {\n        // add the position of the nearby vertex to the new position\n        new_position += current_halfedge-&gt;next()-&gt;vertex()-&gt;position;\n        n++;\n        // move to the next halfedge\n        current_halfedge = current_halfedge-&gt;twin()-&gt;next();\n\n      } while (current_halfedge != start_halfedge);\n</code></pre> <p>When all the surrounding vertices are visited, the new position is updated using the formula provided in the methodology section.</p> C++<pre><code>      if (n == 3)\n      {\n        u = 3.0 / 16.0;\n      }\n      else\n      {\n        u = 3.0 / (8.0 * n);\n      }\n      // calculate the new position\n      new_position = (1 - n * u) * v-&gt;position + u * new_position;\n</code></pre> <p>The above process is repeated for all vertices in the mesh to obtain the updated positions.</p>"},{"location":"Homework2/Section2/Part6/#compute-new-vertex-positions","title":"Compute New Vertex Positions","text":"<p>A new vertex is added at the midpoint of each edge in the mesh. The new position is calculated using the positions of the two vertices connected by the edge and the positions of the vertices of the adjacent triangles.</p> <p>First, the four vertices are obtained by halfedge traversal. C++<pre><code>      // locate the four surrounding vertices\n      VertexIter a = e-&gt;halfedge()-&gt;vertex();\n      VertexIter b = e-&gt;halfedge()-&gt;twin()-&gt;vertex();\n      VertexIter c = e-&gt;halfedge()-&gt;twin()-&gt;next()-&gt;next()-&gt;vertex();\n      VertexIter d = e-&gt;halfedge()-&gt;next()-&gt;next()-&gt;vertex();\n</code></pre></p> <p>Then, the new position is calculated using the formula provided in the methodology section. C++<pre><code>      // calculate the new position for the new vertices\n      e-&gt;newPosition = 3.0 / 8.0 * (a-&gt;position + b-&gt;position) + 1.0 / 8.0 * (c-&gt;position + d-&gt;position);\n</code></pre></p> <p>This step is repeated for all edges in the mesh to obtain the new vertex positions.</p>"},{"location":"Homework2/Section2/Part6/#split-edges","title":"Split Edges","text":"<p>In loop subdivision, every edge in the mesh is split once and only once. To prevent repeated splitting, status of each edge need to be tracked. The starter code provides an `Edge::isNew`` flag to indicate whether the edge is new or not. </p> <p>However, there are multiple definitions of new and old edges in this problem. </p> <ul> <li>Logical New Edge: An edge that is created during the subdivision process. It is not present in the original mesh. When an old edge is split into two parts, these parts are treated as old edges.</li> <li>Chronological New Edge: The edges formed after splitting an old edge are treated as new edges.</li> </ul> <p>For this subsection, both definitions are feasible. However, for the whole problem, only the logical new edge is practical. This will be discussed in the next section.</p> <p>As logical new edge definition cannot decide whether an edge has been split or not, alternative method is used to track the status of each edge. For a newly split edge, it must connect one new vertex and one old vertex. This way, the edge split can be completed within a single iteration of the mesh edges.</p> <p>First, all the current edges and vertices are marked as old ones: C++<pre><code>    // now every edge in the mesh is old edge\n    // set the isNew flag to false for every edge\n    for (EdgeIter e = mesh.edgesBegin(); e != mesh.edgesEnd(); e++)\n    {\n      e-&gt;isNew = false;\n    }\n\n    // now every vertex in the mesh is old vertex\n    // set the isNew flag to false for every vertex\n    for (VertexIter v = mesh.verticesBegin(); v != mesh.verticesEnd(); v++)\n    {\n      v-&gt;isNew = false;\n    }\n</code></pre></p> <p>Then the iteration begins. For specific edges, check if it is a chronological new edge: C++<pre><code>      if (e-&gt;halfedge()-&gt;vertex()-&gt;isNew || e-&gt;halfedge()-&gt;twin()-&gt;vertex()-&gt;isNew)\n      {\n        continue;\n      }\n</code></pre></p> <p>After the edge is split, the chronologically new edges are marked as logically new and old edges: C++<pre><code>      // split the edge\n      Vector3D new_position = e-&gt;newPosition;\n      VertexIter new_vertex = mesh.splitEdge(e);\n      // set the isNew flag to true for the new edges\n      // the original edge, which is now split into two edges, is regarded as a old edge\n      HalfedgeIter current_halfedge = new_vertex-&gt;halfedge();\n      current_halfedge-&gt;edge()-&gt;isNew = false; // one of the original edges\n      current_halfedge = current_halfedge-&gt;twin()-&gt;next();\n      current_halfedge-&gt;edge()-&gt;isNew = true; // one new edge\n      current_halfedge = current_halfedge-&gt;twin()-&gt;next();\n      current_halfedge-&gt;edge()-&gt;isNew = false; // one of the original edges\n      current_halfedge = current_halfedge-&gt;twin()-&gt;next();\n      current_halfedge-&gt;edge()-&gt;isNew = true; // one new edge\n</code></pre></p> <p>The last step is to process the new vertex. Besides setting it as a new vertex, we immediately assign its <code>newPosition</code>. This is because the edge containing its new position might be flipped in the next section and the pointer to it could be lost. C++<pre><code>      // set the isNew flag to true for the new vertex\n      new_vertex-&gt;isNew = true;\n      // set the new position for the new vertex immediately\n      // because the edge containing the new vertex might be flipped, and the new position will be lost\n      new_vertex-&gt;newPosition = new_position;\n</code></pre></p>"},{"location":"Homework2/Section2/Part6/#flip-new-edges","title":"Flip New Edges","text":"<p>The edges that required flipping are the ones that:</p> <ul> <li>Is logically new: This is why the logical new edge definition is used. Chronologically new edges are not necessarily flipped, like the ones that are two parts of a split edge.</li> <li>Connects an old vertex and a new vertex: As mentioned in the methodology section.</li> </ul> <p>The corresponding code is: C++<pre><code>      // check if the edge is a new edge\n      if (!e-&gt;isNew)\n      {\n        continue;\n      }\n\n      // check if the edge connects an old and new vertex\n      if (e-&gt;halfedge()-&gt;vertex()-&gt;isNew==e-&gt;halfedge()-&gt;next()-&gt;vertex()-&gt;isNew)\n      {\n        continue;\n      }\n      else\n      {\n        // flip the edge\n        mesh.flipEdge(e);\n      }\n</code></pre></p>"},{"location":"Homework2/Section2/Part6/#copy-new-vertex-positions","title":"Copy New Vertex Positions","text":"<p>Because both the mesh's original vertices and the ones created by splitting edges have their 'newPosition' set, the final step is to copy the new positions to the original positions. C++<pre><code>    // 5. Copy the new vertex positions into final Vertex::position.\n    // iterate through all vertices\n    for (VertexIter v = mesh.verticesBegin(); v != mesh.verticesEnd(); v++)\n    {\n      // update the position of the old vertex\n      v-&gt;position = v-&gt;newPosition;\n    }\n</code></pre></p>"},{"location":"Homework2/Section2/Part6/#results","title":"Results","text":"<p>The following images are the upsampling result of <code>dae/torus/input.dae</code> with level 0,1,2, and 3 loop subdivision. The mesh is clearly smoother as the level increases.</p> <p> </p> <p>One problem is that the sharp edges are smoothed out. Inspecting loop subdivision from a qualitative perspective, the new position of a vertex is determined by its surrounding vertices. This means if a vertex is distant from its neighbors, its distinct position will be averaged out. So to preserve sharp edges, we need to add more vertex around them to strengthen their influence. The following images are the same mesh but with pre-processed outer faces at upsample level 0 and 2. The outer sharp edges are much better preserved.</p> <p> </p> <p>The same applies to <code>dae/cube.dae</code>. Considering only the face towards us, corner \\(A\\) is averaged by its three neighbors during upsampling, while corner \\(B\\) is averaged by two. If we apply this idea recursively to the triangles created by subdivision, we can see that the influence of corner \\(A\\) is much stronger than corner \\(B\\). This is why corner \\(A\\) is much smoother than corner \\(B\\) in the upsampled mesh.</p> <p></p> <p>This provides a hint to solve the asymmetric problem. We need to ensure the structure of neighboring vertices of the corners is symmetric. A simple pre-processing is used in the following figures. The asymmetry is removed after loop subdivision.</p> <p> </p>"},{"location":"Homework3/Overview/","title":"Homework3: Pathtracer Overview","text":""},{"location":"Homework3/Overview/#overview","title":"Overview","text":"<p>In this homework, we implemented a pathtracer that simulates the global illumination of a scene. The pathtracer is capable of simulating the following effects:</p> <ul> <li>Direct illumination</li> <li>Indirect illumination</li> <li>BVH acceleration</li> <li>Importance sampling</li> <li>Russian roulette</li> </ul> Note for PDF <p>Some features of the website, such as picture lightbox, may not be available in the PDF version. We recommend reading the website directly for the best experience.</p>"},{"location":"Homework3/Part1/","title":"Part1: Ray Generation and Scene Intersection","text":""},{"location":"Homework3/Part1/#overview-the-ray-generation-and-intersection-pipeline","title":"Overview: The ray generation and intersection pipeline","text":"<p>In this part, we implemented the ray generation and intersection pipeline for the Pathtracer. The pipeline consists of the following steps:</p> <pre><code>flowchart TB\n  A[Pixel Sample Generation];\n  B[Camera Ray Generation];\n  C[Ray-Triangle Intersection];\n  D[Ray-Sphere Intersection];\n  A --&gt; B;\n  B --&gt; C;\n  B --&gt; D;</code></pre> <p>The above sections will be explained in detail in the following sections.</p>"},{"location":"Homework3/Part1/#pixel-sample-generation","title":"Pixel Sample Generation","text":""},{"location":"Homework3/Part1/#methodology","title":"Methodology","text":"<p>In the Pathtracer, each pixel of the image is sampled <code>ns_aa</code> times. The input pixel coordinates lie in the unnormalized image space, i.e., the range of the coordinates is from 0 to <code>width</code> and 0 to <code>height</code>. </p> <p></p> <p>Each sample process consists of the following steps:</p> <ol> <li>Generate a random sample point in the pixel space.</li> <li>Normalize the sample point to the camera space.</li> <li>Generate a ray from the camera origin to the sample point.</li> <li>Obtain the returned color and add it to the pixel color.</li> </ol>"},{"location":"Homework3/Part1/#implementation","title":"Implementation","text":"<p>The grid sampler provided in the starter code is used to generate the sample points:</p> C++<pre><code>      // generate a random sample\n      sample = gridSampler-&gt;get_sample();\n</code></pre> <p>As the width and height of the camera space are 1, the sample point is normalized by dividing the sample point by the width and height of the sample buffer:</p> C++<pre><code>      xsample_normalized = ((double)x + sample.x) / sampleBuffer.w;\n      ysample_normalized = ((double)y + sample.y) / sampleBuffer.h;\n</code></pre> <p>Finally, the ray is generated from the camera origin to the sample point, and the returned color is added to the pixel color:</p> C++<pre><code>      // generate a random ray\n      r = camera-&gt;generate_ray(xsample_normalized, ysample_normalized);\n      // trace the ray\n      L_out += est_radiance_global_illumination(r);\n</code></pre> <p>This process is repeated for <code>ns_aa</code> times to obtain the final pixel color. The result is averaged and assigned to the pixel buffer.</p>"},{"location":"Homework3/Part1/#camera-ray-generation","title":"Camera Ray Generation","text":""},{"location":"Homework3/Part1/#methodology_1","title":"Methodology","text":"<p>The given pixel position is in the normalized image space. After obtaining its position in the camera space, the ray direction in the camera space is determined. To generate the ray, this direction is transformed to the world space.</p> <p></p> <p>Transforming pixel position from the normalized image space to the camera space involves translating the pixel position to the camera space and scaling it by the camera width and height. The translation matrix is given by:</p> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; -0.5 \\\\ 0 &amp; 1 &amp; -0.5 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>The scaling matrix is given by:</p> \\[ \\begin{bmatrix} 2 \\times \\tan(\\frac{hFov}{2}) &amp; 0 &amp; 0 \\\\ 0 &amp; 2 \\times \\tan(\\frac{vFov}{2}) &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>The ray direction in the camera space is the vector from the camera position to the sensor position. The sensor position is obtained by changing the third component of the pixel position to -1. </p> <p>The camera-to-world matrix the given and does not need to be computed. </p>"},{"location":"Homework3/Part1/#implementation_1","title":"Implementation","text":"<p>First, the translation matrix <code>move_to_center</code> and scaling matrix <code>scale_to_sensor</code> are defined:</p> C++<pre><code>  // step 1: transform the input image coordinate to the virtual sensor plane coordinate\n  // move the input image coordinate to the center of the image plane\n  Matrix3x3 move_to_center = Matrix3x3(1, 0, -0.5, 0, 1, -0.5, 0, 0, 1);\n  // scale the input image coordinate to the sensor plane coordinate\n  // the virtual sensor plane is 1 unit away from the camera\n  Matrix3x3 scale_to_sensor = Matrix3x3(2 * tan(radians(hFov / 2)), 0, 0, 0, 2 * tan(radians(vFov / 2)), 0, 0, 0, 1);\n</code></pre> <p>The pixel position is transformed to the sensor position by multiplying the translation and scaling matrices:</p> C++<pre><code>  // apply the transformation\n  Vector3D input_image_position = Vector3D(x, y, 1);\n  Vector3D sensor_position = scale_to_sensor * move_to_center * input_image_position;\n</code></pre> <p>The ray direction in the camera space is the vector from the camera position to the sensor position:</p> C++<pre><code>  // step 2: compute the ray direction in camera space\n  // the ray direction is the vector from the camera position to the sensor position\n  // in camera space, camera is at the origin\n  // simply change the third component of the sensor position to -1\n  Vector3D ray_direction_in_camera = {sensor_position.x, sensor_position.y, -1};\n</code></pre> <p>Finally, the ray direction is transformed to the world space, and the ray is initialized.</p> C++<pre><code>  // step 3: transform the ray direction to world space\n  // use the camera-to-world rotation matrix\n  Vector3D ray_direction_in_world = c2w * ray_direction_in_camera;\n\n  // normalize the ray direction\n  ray_direction_in_world.normalize();\n\n  Ray result = Ray(pos, ray_direction_in_world);\n  // initialize the range of the ray\n  result.min_t = nClip;\n  result.max_t = fClip;\n\n  return  result;\n</code></pre>"},{"location":"Homework3/Part1/#ray-triangle-intersection","title":"Ray-Triangle Intersection","text":""},{"location":"Homework3/Part1/#methodology_2","title":"Methodology","text":"<p>We strictly followed the Moller-Trumbore algorithm to implement the ray-triangle intersection. The algorithm is based on the following steps:</p> <p></p>"},{"location":"Homework3/Part1/#implementation_2","title":"Implementation","text":"<p>First, the Barycentric coordinates of the intersection point and its <code>t</code> value are obtained by the Moller-Trumbore algorithm:</p> C++<pre><code>  // Use Moller-Trumbore intersection algorithm\n  Vector3D E_1 = p2 - p1;\n  Vector3D E_2 = p3 - p1;\n  Vector3D S = r.o - p1;\n  Vector3D S_1 = cross(r.d, E_2);\n  Vector3D S_2 = cross(S, E_1);\n\n  double denominator = dot(S_1, E_1);\n\n  if (denominator == 0) {\n    return false;\n  }\n\n  double t = dot(S_2, E_2) / denominator;\n  double b1 = dot(S_1, S) / denominator;\n  double b2 = dot(S_2, r.d) / denominator;\n  double b0 = 1 - b1 - b2;\n</code></pre> <p>Next, the intersection point is checked:</p> <ul> <li>If the Barycentric coordinates are inside the triangle.</li> <li>If the <code>t</code> value is within the range of the ray.</li> </ul> C++<pre><code>  // check if t is within the range\n  if (t &lt; r.min_t || t &gt; r.max_t) {\n    return false;\n  }\n\n  // check if b1, b2, and b3 are within the range\n  if (b1 &lt; 0 || b1 &gt; 1 || b2 &lt; 0 || b2 &gt; 1 || b0 &lt; 0 || b0 &gt; 1) {\n    return false;\n  }\n</code></pre> <p>If the intersection point is valid, the last step is to update the <code>max_t</code> value of the ray and return <code>true</code>.</p> C++<pre><code>  // update the max_t value of the ray\n  r.max_t = t;\n  return true;\n</code></pre>"},{"location":"Homework3/Part1/#ray-sphere-intersection","title":"Ray-Sphere Intersection","text":""},{"location":"Homework3/Part1/#methodology_3","title":"Methodology","text":"<p>For a sphere with center <code>c</code> and radius <code>r</code>, the ray-sphere intersection is calculated by solving the following quadratic equation:</p> \\[ (o + td - c) \\cdot (o + td - c) = r^2 \\] <p>where <code>o</code> is the ray origin, <code>d</code> is the ray direction, and <code>t</code> is the intersection point. The solution to the quadratic equation is given by:</p> \\[ \\begin{align*} t &amp;= \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\\\ a &amp;= d \\cdot d \\\\ b &amp;= 2(d \\cdot (o - c)) \\\\ c &amp;= (o - c) \\cdot (o - c) - r^2 \\end{align*} \\] <p>If the part inside the square root is negative, there is no intersection. For this task, we check:</p> <ul> <li>If there is an intersection.</li> <li>If the <code>t</code> value is within the range of the ray.</li> </ul>"},{"location":"Homework3/Part1/#implementation_3","title":"Implementation","text":"<p>In the <code>test</code> function, whether the ray intersects with the sphere is tested:</p> C++<pre><code>bool Sphere::test(const Ray &amp;r, double &amp;t1, double &amp;t2) const {\n\n  // TODO (Part 1.4):\n  // Implement ray - sphere intersection test.\n  // Return true if there are intersections and writing the\n  // smaller of the two intersection times in t1 and the larger in t2.\n\n  // Implementation by Ruhao Tian starts here\n  // a t^2 + b t + c = 0\n  double a = r.d.norm2();\n  double b = 2 * dot(r.d, r.o - o);\n  double c = (r.o - o).norm2() - r2;\n\n  // check if there's valid solution\n  double delta = b * b - 4 * a * c;\n  if (delta &lt; 0) {\n    return false;\n  }\n\n  // find the two solutions\n  double sqrt_delta = sqrt(delta);\n  t1 = (-b - sqrt_delta) / (2 * a);\n  t2 = (-b + sqrt_delta) / (2 * a);\n\n  return true;\n\n}\n</code></pre> <p>The function <code>has_intersection</code> will call <code>test</code> and check further if the intersection point is within the range of the ray:</p> C++<pre><code>bool Sphere::has_intersection(const Ray &amp;r) const {\n\n  // TODO (Part 1.4):\n  // Implement ray - sphere intersection.\n  // Note that you might want to use the the Sphere::test helper here.\n\n  // Implementation by Ruhao Tian starts here\n  double t1, t2;\n  if (!test(r, t1, t2)) {\n    return false;\n  }\n\n  // test if t1 is in the valid range\n  if (t1 &gt;= r.min_t &amp;&amp; t1 &lt;= r.max_t) {\n    r.max_t = t1;\n    return true;\n  }\n  else if (t2 &gt;= r.min_t &amp;&amp; t2 &lt;= r.max_t) {\n    r.max_t = t2;\n    return true;\n  }\n\n  return false;\n}\n</code></pre>"},{"location":"Homework3/Part1/#results","title":"Results","text":"<p>After implementing the ray generation and intersection pipeline, we obtained the following results:</p> <p>CBspheres.dae </p> <p>CBgems.dae </p> <p>CBcoil.dae </p>"},{"location":"Homework3/Part2/","title":"Part 2: Bounding Volume Hierarchy","text":""},{"location":"Homework3/Part2/#constructing-the-bvh","title":"Constructing the BVH","text":""},{"location":"Homework3/Part2/#methodology","title":"Methodology","text":"<p>The method I use to construct the BVH (Bounding Volume Hierarchy) is based on splitting at the centroid as the midpoint. The specific choice of the splitting axis is completely random. This ensures that, in the vast majority of cases, the range of leaves will not be too flat.</p> <p>The specific steps are as follows:</p> <ol> <li>Select Split Axis: Randomly choose one from the \\(x\\), \\(y\\), \\(z\\) axes as the split axis.</li> <li>Calculate Centroid for Selected Axis: Find the position of the centroid along the selected axis</li> <li>Sort Pointers According to Coordinates on Selected Axis: Arrange the pointers at this point in order of their corresponding coordinates along the selected axis, from smallest to largest.</li> <li>Splitting Nodes: The left subtree contains the first half of the pointers. The right subtree contains the second half of the pointers. Recur this process until the maximum value of nodes is reached.</li> </ol> <p></p>"},{"location":"Homework3/Part2/#implementation","title":"Implementation","text":"<p>Determine whether the \\(max_leaf_size\\) has been reached.</p> C++<pre><code>if (end - start + 1 &lt;= max_leaf_size){\n    node-&gt;start = start;\n    node-&gt;end = end;\n    node-&gt;l = node-&gt;r = NULL;\n    return node;\n    }\n</code></pre> <p>Randomly choose one axis. 0 for x, 1 for y and 2 for z. C++<pre><code>int axis=rand()%3;\n</code></pre></p> <p>Sort pointers according to coordinates on the selected axis.</p> C++<pre><code>  std::sort(start, end, [axis](Primitive *a, Primitive *b)\n    { return a-&gt;get_bbox().centroid()[axis] &lt; b-&gt;get_bbox().centroid()[axis]; });\n</code></pre> <p>Splitting nodes.</p> C++<pre><code>  left_end = start + (end - start) / 2;\n  right_start = left_end;\n  node-&gt;l = construct_bvh(start, left_end, max_leaf_size);\n  node-&gt;r = construct_bvh(right_start, end, max_leaf_size);\n</code></pre>"},{"location":"Homework3/Part2/#intersecting-the-bounding-box","title":"Intersecting the Bounding Box","text":"<p>For most situations: Check the entry and exit times. If the entry time and exit time are in the range between t0 and t1, return True. In addition, the entry time is the maximum of the entry times along each axis, and the exit time is the minimum of the exit times along each axis. Otherwise, return False.</p> <p></p> <p>Special Case 1:</p> <p>If the ray is parallel to one or more axes, let it(or one of them) as axis m. If the value of axis m at the starting position of the ray is within the range of the node, then consider the entry time as negative infinity and the exit time as positive infinity at axis m. Otherwise, return False.</p> <p>Special Case 2: If the origin of the ray is inside the bounding box, return True.</p> <p>Special Case 3: If the ray stops in the bounding box, return True.</p>"},{"location":"Homework3/Part2/#implementation_1","title":"Implementation","text":"<p>Consider the common situations and the Special Case 1. t[/][0] means the enter time at axis /. t[/][1] means the exit time at axis /. t[0] means axis x. Similarly, t[1] for y and t[2] for z.</p> <p>C++<pre><code>  double t[3][2];\n  if(r.d.x==0){\n    if(r.o.x&lt;min.x||r.o.x&gt;max.x)return false;\n    t[0][0]=-99999;\n    t[0][1]=999999;\n  }\n  else{\n    t[0][0]=std::min((min.x-r.o.x)/r.d.x,(max.x-r.o.x)/r.d.x);\n    t[0][1]=std::max((min.x-r.o.x)/r.d.x,(max.x-r.o.x)/r.d.x);\n  }\n  if(r.d.y==0){\n    if(r.o.y&lt;min.y||r.o.y&gt;max.y)return false;\n    t[0][0]=-99999;\n    t[0][1]=999999;\n  }\n  else{\n    t[1][0]=std::min((min.y-r.o.y)/r.d.y,(max.y-r.o.y)/r.d.y);\n    t[1][1]=std::max((min.y-r.o.y)/r.d.y,(max.y-r.o.y)/r.d.y);\n  }\n  if(r.d.z==0){\n    if(r.o.z&lt;min.z||r.o.z&gt;max.z)return false;\n    t[0][0]=-99999;\n    t[0][1]=999999;\n  }\n  else{\n    t[2][0]=std::min((min.z-r.o.z)/r.d.z,(max.z-r.o.z)/r.d.z);\n    t[2][1]=std::max((min.z-r.o.z)/r.d.z,(max.z-r.o.z)/r.d.z);\n  }\n</code></pre> Initialize the entry and exit times as described earlier.</p> C++<pre><code>  double t_enter = std::max(std::max(t[0][0], t[1][0]), t[2][0]);\n  double t_exit = std::min(std::min(t[0][1], t[1][1]), t[2][1]);\n</code></pre> <p>Check if the intersection is in the range of t0 and t1</p> <p>C++<pre><code>  if (t_exit &gt; t_enter &amp;&amp; t_exit &gt; 0)\n  {\n    if (t_enter &gt; t0 &amp;&amp; t_enter &lt; t1)\n    {\n      t0 = t_enter;\n      return true;\n    }\n  }\n</code></pre> Consider the Special Case 2.</p> <p>C++<pre><code>  if (r.o.x &gt;= min.x &amp;&amp; r.o.x &lt;= max.x &amp;&amp; r.o.y &gt;= min.y &amp;&amp; r.o.y &lt;= max.y &amp;&amp; r.o.z &gt;= min.z &amp;&amp; r.o.z &lt;= max.z)\n  {\n    return true;\n  }\n</code></pre> Consider the Special Case 3.</p> C++<pre><code>  Vector3D end = r.o + r.d * r.max_t;\n  if (end.x &gt;= min.x &amp;&amp; end.x &lt;= max.x &amp;&amp; end.y &gt;= min.y &amp;&amp; end.y &lt;= max.y &amp;&amp; end.z &gt;= min.z &amp;&amp; end.z &lt;= max.z)\n  {\n    return true;\n  }\n</code></pre>"},{"location":"Homework3/Part2/#intersecting-the-bvh","title":"Intersecting the BVH","text":""},{"location":"Homework3/Part2/#methodology_1","title":"Methodology","text":"<p>The core idea is as follows: If it is not a leaf node, recursively check the left and right child nodes. If it is a leaf node, use the function generated in the previous step to check for intersection.</p>"},{"location":"Homework3/Part2/#implementation_2","title":"Implementation","text":""},{"location":"Homework3/Part2/#bvhaccelhas_intersection","title":"BVHAccel:has_intersection","text":"<p>If it is a leaf node.</p> <p>'''cpp   if (node-&gt;isLeaf()){     // check if the ray intersects with a bounding box of the node     if (!node-&gt;bb.intersect(ray, t0, t1))return false;     // has intersection, check all primitives in the node     for (auto p = node-&gt;start; p != node-&gt;end; p++){       total_isects++;       if ((*p)-&gt;has_intersection(ray))return true;       }     return false;     } Text Only<pre><code>If it is not a leaf node, recursively check the left and right child nodes.\n\n```cpp\n  bool left_result, right_result;\n  left_result = has_intersection(ray, node-&gt;l);\n  right_result = has_intersection(ray, node-&gt;r);\n</code></pre></p>"},{"location":"Homework3/Part2/#bvhaccelintersect","title":"BVHAccel:intersect","text":"<p>This is quite similar to the approach discussed earlier, so I will provide the code directly. More detailed explanations are provided in the comments.</p> C++<pre><code>      double t0 = ray.min_t, t1 = ray.max_t;\n\n      // check if current node is leaf node\n      if (node-&gt;isLeaf())\n      {\n        // check if the ray intersects with bounding box of the node\n        if (!node-&gt;bb.intersect(ray, t0, t1))\n          return false;\n\n        // has intersection, check all primitives in the node\n        bool has_isect = false;\n        for (auto p = node-&gt;start; p != node-&gt;end; p++)\n        {\n          total_isects++;\n          if ((*p)-&gt;has_intersection(ray))\n          {\n            has_isect = true;\n\n            // check the t value of the intersection\n            Intersection temp_isect;\n            (*p)-&gt;intersect(ray, &amp;temp_isect);\n\n            if (temp_isect.t &lt; i-&gt;t)\n            {\n              i-&gt;t = temp_isect.t;\n              i-&gt;primitive = temp_isect.primitive;\n              i-&gt;n = temp_isect.n;\n              i-&gt;bsdf = temp_isect.bsdf;\n            }\n          }\n        }\n        return has_isect;\n      }\n\n      // if current node is not leaf node\n      // check if the ray intersects with bounding box of the node\n      if (!node-&gt;bb.intersect(ray, t0, t1))\n        return false;\n\n      // has intersection, check left and right child\n      bool left_result, right_result;\n      left_result = intersect(ray, i, node-&gt;l);\n      right_result = intersect(ray, i, node-&gt;r);\n      // NEVER use 'return intersect(ray, i, node-&gt;l) || intersect(ray, i, node-&gt;r);'\n      // because the right child might not be checked if the left child has intersection\n      return left_result || right_result;\n</code></pre>"},{"location":"Homework3/Part2/#result","title":"Result","text":"<p>Images with normal shading for a few large .dae files that I can only render with BVH acceleration.</p> <p></p> <p></p> <p>I will compare running the following code with and without building the BVH:</p> <p>C++<pre><code>  ./pathtracer -t 8 -r 800 600 -f cow.png ../dae/meshedit/cow.dae\n</code></pre> Without BVH:</p> <p></p> <p>It costs about 13s.</p> <p>With BVH:</p> <p></p> <p>It costs less than 1s.</p> <p>For the other images:</p>"},{"location":"Homework3/Part2/#beastdae","title":"beast.dae","text":"Text Only<pre><code>            without BVH\n</code></pre> Text Only<pre><code>            with BVH\n</code></pre>"},{"location":"Homework3/Part2/#beetledae","title":"beetle.dae","text":"Text Only<pre><code>            without BVH\n</code></pre> Text Only<pre><code>            with BVH\n</code></pre> <p>Summary: Using BVH can be nearly a thousand times faster compared to not using it. For example, rendering beast.dae without BVH takes 3 minutes, while using BVH takes less than half a second. The reason for this is that this structure reduces the number of intersection tests needed to find the intersection between a ray and an object in the scene.</p>"},{"location":"Homework3/Part3/","title":"Part3: Direct Illumination","text":""},{"location":"Homework3/Part3/#overview","title":"Overview","text":"<p>Global illumination radiance from direct illumination is estimated in this part. The direct illumination consists of zero-bounce and one-bounce light transport, and the latter could be estimated by uniformly sampling the hemisphere or importance sampling the BRDF. The structure of the pipeline is as follows:</p> <pre><code>flowchart TB\n    A[Direct Illumination of Single Ray];\n    B[Zero-Bounce Light Transport];\n    C[One-Bounce Light Transport];\n    D[Uniform Sampling];\n    E[Importance Sampling];\n    A --&gt; B;\n    A --&gt; C;\n    C --&gt; D;\n    C --&gt; E;</code></pre> <p>The above sections will be explained in detail in the following sections.</p>"},{"location":"Homework3/Part3/#zero-bounce-illumination","title":"Zero-Bounce Illumination","text":""},{"location":"Homework3/Part3/#methodology","title":"Methodology","text":"<p>Zero-bounce illumination of a single ray is estimated by adding the emitted radiance from the light sources to the pixel color. The emitted radiance here only considers the light sources and does not consider the indirect illumination from the scene. The rendering equation for zero-bounce illumination is given by:</p> \\[ L_o(p, \\omega_o) = L_e(p, \\omega_o) \\] <p>where \\(L_o(p, \\omega_o)\\) is the radiance leaving the point \\(p\\) in the direction \\(\\omega_o\\), and \\(L_e(p, \\omega_o)\\) is the emitted radiance from the light sources.</p>"},{"location":"Homework3/Part3/#implementation","title":"Implementation","text":"<p>In the starter code, the emitted radiance from the intersected light source of the ray is encapsulated in <code>BSDF::get_emission()</code>. The emitted radiance is added to the pixel color and returned:</p> C++<pre><code>    // obtain the emission color of the intersection\n    Vector3D L_out = isect.bsdf-&gt;get_emission();\n\n    return L_out;\n</code></pre>"},{"location":"Homework3/Part3/#uniform-hemisphere-sampling","title":"Uniform Hemisphere Sampling","text":""},{"location":"Homework3/Part3/#methodology_1","title":"Methodology","text":"<p>In physical-based rendering, the reflected radiance from the surface is calculated by integrating the product of the BRDF and the incident radiance over the hemisphere:</p> \\[ L_o(p, \\omega_o) = \\int_{\\Omega} f(p, \\omega_i, \\omega_o) L_i(p, \\omega_i) \\cos(\\theta_i) d\\omega_i \\] <p>where \\(L_o(p, \\omega_o)\\) is the radiance leaving the point \\(p\\) in the direction \\(\\omega_o\\), \\(f(p, \\omega_i, \\omega_o)\\) is the BRDF, \\(L_i(p, \\omega_i)\\) is the incident radiance from the direction \\(\\omega_i\\), and \\(\\theta_i\\) is the angle between the normal and the incident direction. </p> <p>The integral is approximated by sampling the hemisphere uniformly and averaging the results using Monte Carlo integration:</p> \\[ L_o(p, \\omega_o) \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(p, \\omega_i, \\omega_o) L_i(p, \\omega_i) \\cos(\\theta_i) \\] <p>where \\(N\\) is the number of samples. For uniform sampling, the possibility function is given by:</p> \\[ p(\\omega_i) = \\frac{1}{2\\pi} \\] <p>The incident radiance can be obtained by zero-bounce illumination, and \\(\\cos(\\theta_i)\\) is the dot product between the normal and the incident direction.</p> <p>The uniform hemisphere sampling calculates \\(L_o(p, \\omega_o)\\) by sampling the hemisphere uniformly and averaging to produce the final result.</p>"},{"location":"Homework3/Part3/#implementation_1","title":"Implementation","text":"<p>First, a random direction is sampled in the hemisphere space, and its corresponding vector in world space is also prepared to calculate \\(\\cos(\\theta_i)\\):</p> C++<pre><code>      // sample a random direction in the hemisphere\n      wi_sample_o = hemisphereSampler-&gt;get_sample();\n\n      // transform the sample to world space\n      wi_sample_w = o2w * wi_sample_o;\n      wi_sample_w.normalize();\n      // find cos_theta, which is the dot product of the sample direction and the normal\n      cos_theta = dot(wi_sample_w, isect.n) / wi_sample_o.norm() / isect.n.norm();\n</code></pre> <p>Then a sample ray is generated in the direction of the sampled vector, and the incident radiance is calculated by zero-bounce illumination if the ray intersects with the scene:</p> C++<pre><code>      // find the light coming from the sample direction\n      // first construct a ray from the hit point to the sample direction\n      ray = Ray(hit_p, wi_sample_w);\n      ray.min_t = EPS_F;\n      Intersection sample_isect;\n\n      // then check if the ray intersects with any object\n      if (bvh-&gt;intersect(ray, &amp;sample_isect))\n      {\n\n        // use zero bounce radiance to calculate the light coming from the sample direction\n        wi_sample_incoming_light = zero_bounce_radiance(ray, sample_isect);\n      }\n      else\n      {\n        // else there's no light coming from the sample direction\n        wi_sample_incoming_light = Vector3D(0, 0, 0);\n      }\n</code></pre> <p>Finally, the partial result is calculated with Monte Carlo integration and added to the pixel color:</p> C++<pre><code>      // for uniform sampling, the probability of each sample is 1 / (2 * PI)\n      // now put everything together\n      L_out += (1.0 / (double)num_samples) * (cos_theta * isect.bsdf-&gt;f(w_out, wi_sample_o) * wi_sample_incoming_light * (2 * PI));\n</code></pre> <p>This process is repeated for <code>num_samples</code> times, and the final result is returned.</p>"},{"location":"Homework3/Part3/#importance-sampling","title":"Importance Sampling","text":""},{"location":"Homework3/Part3/#methodology_2","title":"Methodology","text":"<p>Unlike uniform sampling, importance sampling only samples in the direction of light sources. The Monte Carlo integration is also implemented. The render equation is the same as the uniform sampling, but the possibility function is different.</p>"},{"location":"Homework3/Part3/#implementation_2","title":"Implementation","text":"<p>The importance sampler will iterate through all the light sources in the scene. Each light source is first checked if it is a delta light(point light source), this is because the result of a point light source is deterministic and only needs to be calculated once. The area light sources are sampled based on their area, a <code>num_total_samples</code> is maintained to perform the normalizing step in the Monte Carlo integration.</p> C++<pre><code>      // sample the light\n      if (light-&gt;is_delta_light())\n      {\n        num_samples = 1;\n      }\n      else\n      {\n        num_samples = ns_area_light;\n      }\n      num_total_samples += num_samples;\n</code></pre> <p>For each sample of each light source, the starter code provides the <code>sample_L</code> function to sample as well as calculates the possibility function.</p> C++<pre><code>      // sample the light\n      for (int i = 0; i &lt; num_samples; i++)\n      {\n        wi_sample_incoming_light = light-&gt;sample_L(hit_p, &amp;wi_sample_w, &amp;dist_to_light, &amp;pdf);\n</code></pre> <p>Again, a ray is generated in the sample direction. However, in importance sampling it is checked that the ray does not hit anything until it reaches the light source.</p> C++<pre><code>        // generate a ray from the hit point to the light, check if it hit anything\n        ray = Ray(hit_p, wi_sample_w);\n        ray.min_t = EPS_F;\n        ray.max_t = dist_to_light - EPS_F; // don't forget to subtract EPS_F to avoid self-intersection\n\n        if (bvh-&gt;has_intersection(ray))\n        {\n          continue;\n        }\n</code></pre> <p>If the ray does not hit anything, the partial result is calculated the same way as in uniform sampling and added to the pixel color.</p> C++<pre><code>        // calculate cos_theta\n        cos_theta = dot(wi_sample_w, isect.n) / wi_sample_w.norm() / isect.n.norm();\n        // calculate the sample direction in object space\n        wi_sample_o = w2o * wi_sample_w;\n\n        // put everything together\n        L_out += (cos_theta * isect.bsdf-&gt;f(w_out, wi_sample_o) * wi_sample_incoming_light) / pdf;\n</code></pre> <p>Finally, the result is normalized by <code>num_total_samples</code> and returned.</p>"},{"location":"Homework3/Part3/#one-bounce-illumination-and-direct-illumination","title":"One-Bounce Illumination and Direct Illumination","text":"<p>One-Bounce Illumination function calls different sampling methods based on user selection:</p> C++<pre><code>  Vector3D PathTracer::one_bounce_radiance(const Ray &amp;r,\n                                           const Intersection &amp;isect)\n  {\n    // TODO: Part 3, Task 3\n    // Returns either the direct illumination by hemisphere or importance sampling\n    // depending on `direct_hemisphere_sample`\n\n    // Implementation by Ruhao Tian starts here\n\n    if (direct_hemisphere_sample)\n    {\n      return estimate_direct_lighting_hemisphere(r, isect);\n    }\n    else\n    {\n      return estimate_direct_lighting_importance(r, isect);\n    }\n  }\n</code></pre> <p>The direct illumination is estimated by adding the zero-bounce and one-bounce illumination to the pixel color:</p> C++<pre><code>    // add direct illumination to L_out\n    L_out += zero_bounce_radiance(r, isect);\n\n    // TODO (Part 3): Return the direct illumination.\n    // add one bounce radiance to L_out\n    L_out += one_bounce_radiance(r, isect);\n</code></pre>"},{"location":"Homework3/Part3/#results","title":"Results","text":"<p>The following images show the results of the direct illumination with uniform sampling and importance sampling.</p> <p>CBbunny.dae</p> <p>Uniform Sampling</p> <p></p> <p>Importance Sampling</p> <p></p> <p>CBspheres_lambertian.dae</p> <p>Uniform Sampling</p> <p></p> <p>Importance Sampling</p> <p></p> <p>From the images, it can be observed that the importance sampling produces a smoother result than the uniform sampling. This is because, given the same amount of samples per pixel and per light source, the importance sampling will concentrate more samples on the light sources, which is more efficient than uniformly sampling the hemisphere. Uniform sampling, on the other hand, will waste samples on the directions that do not contribute to the final result, which is why the result is noisier.</p> <p>The following images are the results of the importance sampling with 1, 4, 16, and 64 light rays and 1 sample per pixel of the <code>CBspheres_lambertian.dae</code> scene.</p> <p> </p> <p>According to the images, the result becomes smoother as the number of light rays increases. This is because the more light rays are sampled, the more accurate the result will be. However, the result will also be more noisy if the number of samples per pixel is low. This is why the result is still noisy with 64 light rays and 1 sample per pixel. The result will be smoother if the number of samples per pixel is increased, but it will also be more computationally expensive. Therefore, the number of samples per pixel should be chosen based on the trade-off between the quality of the result and the computational cost.</p>"},{"location":"Homework3/Part4/","title":"Part4: Global Illumination","text":""},{"location":"Homework3/Part4/#sampling-with-diffuse-bsdf","title":"Sampling with Diffuse BSDF","text":""},{"location":"Homework3/Part4/#methodology","title":"Methodology","text":"<p>Using \\(sampler.get_sample(pdf)\\), a random sample direction is obtained from the cosine-weighted hemisphere distribution. Next, the obtained sample direction \\(wi\\) is used as the incoming light direction, and the \\(f\\) function is called to compute the BSDF evaluation of the diffuse material at \\((wo, *wi)\\).</p>"},{"location":"Homework3/Part4/#implementation","title":"Implementation","text":"<p>The function \\(f\\) is defined in Part 3.</p> C++<pre><code>  *wi = sampler.get_sample(pdf);\n  return f(wo, *wi);\n</code></pre>"},{"location":"Homework3/Part4/#global-illumination-with-up-to-n-bounces-of-light","title":"Global Illumination with up to N Bounces of Light","text":""},{"location":"Homework3/Part4/#methodology_1","title":"Methodology","text":"<p>The following is the rendering equation.</p> <p></p> <p>Simplify:</p> <p></p> <p></p> <p>Solve the equation:</p> <p></p>"},{"location":"Homework3/Part4/#implementation_1","title":"Implementation","text":""},{"location":"Homework3/Part4/#at_least_one_bounce_radiance","title":"at_least_one_bounce_radiance","text":"<p>If \\(r.depth == max_ray_depth - 1\\), this means the maximum depth has been reached. L_out just needs to add the value of this diffuse reflection, which is the function \\(one_bounce_radiance\\).</p> <p>C++<pre><code>  if (isAccumBounces || r.depth == max_ray_depth - 1)\n    {\n      L_out += one_bounce_radiance(r, isect);\n    }\n</code></pre> Check if the maximum depth is reached. If true, return the L_out. Otherwise, continue operating the rest of the code.</p> C++<pre><code>  if (r.depth &gt;= max_ray_depth - 1)\n    {\n    return L_out;\n    }\n</code></pre> <p>Now, calculate the radiance from extra bounces. First, obtain a ray from the hit point to the sample direction. </p> C++<pre><code>  Vector3D wi_sample_o, wi_sample_w;\n  double pdf;\n  Vector3D wi_sample_incoming_light;\n  Ray ray;\n  Intersection isect_light;\n  // sample the next direction\n  isect.bsdf-&gt;sample_f(w_out, &amp;wi_sample_o, &amp;pdf);\n  // transform the sample direction to object space\n  wi_sample_w = o2w * wi_sample_o;\n  // generate a ray from the hit point to the sample direction\n  double cos_theta = dot(wi_sample_w, isect.n) / wi_sample_w.norm() / isect.n.\n    norm();\n  ray = Ray(hit_p, wi_sample_w);\n  ray.depth = r.depth + 1;\n  ray.min_t = EPS_F;\n</code></pre> <p>Next, recurse this equation until reaching max_ray_depth(this was checked before).</p> C++<pre><code>  if (bvh-&gt;intersect(ray, &amp;isect_light))\n  {\n    // calculate the radiance from the extra bounces\n    L_out += (isect.bsdf-&gt;f(w_out, wi_sample_o) * at_least_one_bounce_radiance    (ray, isect_light) * cos_theta / pdf);\n  }\n</code></pre>"},{"location":"Homework3/Part4/#est_radiance_global_illumination","title":"est_radiance_global_illumination","text":"<p>Just need to accumulate the result each time.</p> C++<pre><code>  if (max_ray_depth &gt; 0)\n    {\n      L_out += at_least_one_bounce_radiance(r, isect);\n    } \n</code></pre>"},{"location":"Homework3/Part4/#global-illumination-with-russian-roulette","title":"Global Illumination with Russian Roulette","text":""},{"location":"Homework3/Part4/#methodology_2","title":"Methodology","text":"<p>Now we have set a probability between 0.3 and 0.4 (implemented as 0.35) as the probability for early termination. Due to the influence of probability, we need to normalize the result. The core idea is shown in the diagram below.</p> <p></p>"},{"location":"Homework3/Part4/#implementation_2","title":"Implementation","text":"<p>We just need to make some small adjustments to the function \\(at_least_one_bounce_radiance\\). The adjusted code is as follows.</p> <p>First, set the possibility.</p> <p>C++<pre><code>  double possibility = 0.35;\n</code></pre> Next, check whether to terminate early.</p> C++<pre><code>  if (bvh-&gt;intersect(ray, &amp;isect_light) &amp;&amp; coin_flip(possibility)){\n\n  //Here, a lot of code has been omitted.\n\n  return L_out;\n  }\n</code></pre> <p>At last, normalize. Note that the code below has an additional parameter \\(probability\\).</p> C++<pre><code>  L_out += (isect.bsdf-&gt;f(w_out, wi_sample_o) * at_least_one_bounce_radiance(ray, isect_light) * cos_theta / pdf) / possibility;\n</code></pre> <p>Finally, here is the overall code. This is very similar to the previous section, so you don't need to pay too much attention to these codes.</p> C++<pre><code>  if (isAccumBounces || r.depth == max_ray_depth - 1)\n    {\n      L_out += one_bounce_radiance(r, isect);\n    }\n\n  // check if the maximum depth is reached\n  if (r.depth &gt;= max_ray_depth - 1)\n    {\n      return L_out;\n    }\n\n  // if not, calculate the radiance from extra bounces\n  Vector3D wi_sample_o, wi_sample_w;\n  double pdf;\n  Vector3D wi_sample_incoming_light;\n  Ray ray;\n  Intersection isect_light;\n  // sample the next direction\n  isect.bsdf-&gt;sample_f(w_out, &amp;wi_sample_o, &amp;pdf);\n  // transform the sample direction to object space\n  wi_sample_w = o2w * wi_sample_o;\n  // generate a ray from the hit point to the sample direction\n  double cos_theta = dot(wi_sample_w, isect.n) / wi_sample_w.norm() / isect.n.norm();\n  ray = Ray(hit_p, wi_sample_w);\n  ray.depth = r.depth + 1;\n  ray.min_t = EPS_F;\n  if (bvh-&gt;intersect(ray, &amp;isect_light) &amp;&amp; coin_flip(possibility))\n    {\n    // calculate the radiance from the extra bounces\n    L_out += (isect.bsdf-&gt;f(w_out, wi_sample_o) * at_least_one_bounce_radiance(ray, isect_light) * cos_theta / pdf) / possibility;\n    }\n  return L_out;\n</code></pre>"},{"location":"Homework3/Part4/#results","title":"Results","text":""},{"location":"Homework3/Part4/#here-are-some-direct-and-indirect-illumination","title":"Here are some direct and indirect illumination.","text":""},{"location":"Homework3/Part4/#direct","title":"Direct","text":""},{"location":"Homework3/Part4/#indirectonly-sum-m2345","title":"Indirect(only sum m=2,3,4,5)","text":""},{"location":"Homework3/Part4/#compare","title":"Compare","text":"<p>We can find that the direct illumination is more smooth and lighter than the indirect one.</p>"},{"location":"Homework3/Part4/#here-are-the-results-for-rendering-the-mth-bounce-of-light-with-max_ray_depth-set-to-0-1-2-3-4-and-5-and-isaccumbouncesfalse-for-cbbunnydae","title":"Here are the results for rendering the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5, and isAccumBounces=false, for CBbunny.dae","text":""},{"location":"Homework3/Part4/#m0","title":"m=0","text":""},{"location":"Homework3/Part4/#m1","title":"m=1","text":""},{"location":"Homework3/Part4/#m2","title":"m=2","text":""},{"location":"Homework3/Part4/#m3","title":"m=3","text":""},{"location":"Homework3/Part4/#m4","title":"m=4","text":""},{"location":"Homework3/Part4/#m5","title":"m=5","text":""},{"location":"Homework3/Part4/#reason","title":"Reason","text":"<p>This is because each time the light reflects off a surface, some of its energy is absorbed or scattered, leading to an overall decrease in brightness with each subsequent bounce. As the number of reflections increases for the second and third bounces of light, the intensity of light diminishes, resulting in the images becoming progressively darker. </p> <p>The main reason it improves the quality of rendered images is that multiple reflections simulate the paths of real light in the physical world, making the images more closely resemble what we see in reality. This enhances the realism of the images. For example, multiple reflections result in more accurate and realistic shadows, which contribute to a more lifelike appearance.</p>"},{"location":"Homework3/Part4/#here-are-the-results-for-rendered-views-with-max_ray_depth-set-to-0-1-2-3-4-and-5-for-cbbunnydae","title":"Here are the results for rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5  for CBbunny.dae","text":""},{"location":"Homework3/Part4/#m0_1","title":"m=0","text":""},{"location":"Homework3/Part4/#m1_1","title":"m=1","text":""},{"location":"Homework3/Part4/#m2_1","title":"m=2","text":""},{"location":"Homework3/Part4/#m3_1","title":"m=3","text":""},{"location":"Homework3/Part4/#m4_1","title":"m=4","text":""},{"location":"Homework3/Part4/#m5_1","title":"m=5","text":""},{"location":"Homework3/Part4/#compare_1","title":"Compare","text":"<p>We can find that as m increases, the picture becomes brighter and brighter.</p>"},{"location":"Homework3/Part4/#here-are-the-results-for-outputing-the-russian-roulette-rendering-with-max_ray_depth-set-to-0-1-2-3-4-and-100-for-cbbunnydae","title":"Here are the results for outputing the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100  for CBbunny.dae","text":""},{"location":"Homework3/Part4/#m0_2","title":"m=0","text":""},{"location":"Homework3/Part4/#m1_2","title":"m=1","text":""},{"location":"Homework3/Part4/#m2_2","title":"m=2","text":""},{"location":"Homework3/Part4/#m3_2","title":"m=3","text":""},{"location":"Homework3/Part4/#m4_2","title":"m=4","text":""},{"location":"Homework3/Part4/#m100","title":"m=100","text":""},{"location":"Homework3/Part4/#the-following-are-rendering-pictures-of-the-same-scene-with-different-sampling-rates","title":"The following are rendering pictures of the same scene with different sampling rates.","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate1","title":"sample-per-pixel rate=1","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate2","title":"sample-per-pixel rate=2","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate4","title":"sample-per-pixel rate=4","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate8","title":"sample-per-pixel rate=8","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate16","title":"sample-per-pixel rate=16","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate64","title":"sample-per-pixel rate=64","text":""},{"location":"Homework3/Part4/#sample-per-pixel-rate1024","title":"sample-per-pixel rate=1024","text":""},{"location":"Homework3/Part4/#compare_2","title":"Compare","text":"<p>As the sample-per-pixel rate increases, the image will become smoother and smoother. At the same time, the brightness will also increase slightly.</p>"},{"location":"Homework3/Part5/","title":"Part5: Adaptive Sampling","text":""},{"location":"Homework3/Part5/#methodology","title":"Methodology","text":"<p>The high-level idea of adaptive sampling is to reduce the sampling number of pixels that converge to a certain color quickly and focus on the pixels that take longer to converge. A variable \\(I\\) is maintained to measure the convergence of the pixel:</p> \\[ I = 1.96 \\frac{\\sigma}{\\sqrt{n}} \\] <p>where \\(\\sigma\\) is the standard deviation of the pixel color samples, and \\(n\\) is the number of samples.</p> <p>\\(I\\) decreases as the number of samples increases and the standard deviation decreases. When \\(I\\) is less than a certain threshold, the pixel is considered to have converged, and the sampling process stops. The threshold is given by:</p> \\[ I_{\\text{threshold}} = \\text{maxTolerance} \\times \\mu \\] <p>where \\(\\mu\\) is the mean of the pixel color samples, and \\(\\text{maxTolerance}\\) is a user-defined parameter, which is set to 0.05 by default.</p>"},{"location":"Homework3/Part5/#implementation","title":"Implementation","text":"<p>To calculate convergence, two variables <code>total_illumination</code> and <code>total_illumination_squared</code> are maintained:</p> \\[ \\begin{aligned} \\text{total_illumination} &amp;= \\sum_{k=1}^{n} x_{k} \\\\ \\text{total_illumination_squared} &amp;= \\sum_{k=1}^{n} x_{k}^{2} \\end{aligned} \\] <p>where \\(x_{k}\\) is the illumination of sample \\(k\\).</p> <p>For one sample, first, the normal sample process mentioned in Part1 is executed:</p> C++<pre><code>        // generate a random sample\n        current_sample = gridSampler-&gt;get_sample();\n        // don't forget to normalize the sample\n        xsample_normalized = ((double)x + current_sample.x) / sampleBuffer.w;\n        ysample_normalized = ((double)y + current_sample.y) / sampleBuffer.h;\n        // generate a random ray\n        current_ray = camera-&gt;generate_ray(xsample_normalized, ysample_normalized);\n        // trace the ray\n        current_radiance = est_radiance_global_illumination(current_ray);\n        L_out += current_radiance;\n</code></pre> <p>After the normal sample process, the illumination of the current sample is added to <code>total_illumination`` and</code>total_illumination_squared`:</p> C++<pre><code>        current_illumination = current_radiance.illum();\n        total_illumination += current_illumination;\n        total_illumination_squared += current_illumination * current_illumination;\n</code></pre> <p>For more efficient processing, the convergence is tested every <code>samplesPerBatch</code> samples:</p> C++<pre><code>        if (i % samplesPerBatch == 0)\n        {\n          // check convergence of the pixel\n          mean = total_illumination / (i + 1);\n\n          deviation = sqrt((total_illumination_squared - (i + 1) * mean * mean) / i);\n\n          I = 1.96 * deviation / sqrt(i + 1);\n\n          if (I &lt; maxTolerance * mean)\n          {\n            break;\n          }\n        }\n</code></pre> <p>If the pixel color is difficult to converge, it is sampled at most <code>ns_aa</code> time, which is the sample number in normal sampling in Part 1.</p> <p>After the sample process is finished, total radiance <code>L_out</code> is normalized by the actual sample time <code>i</code> and assigned to the sample buffer:</p> C++<pre><code>      // take the average of all the samples\n      L_out /= i + 1;\n      // update the sample buffer\n      sampleBuffer.update_pixel(L_out, x, y);\n      sampleCountBuffer[x + y * sampleBuffer.w] = i + 1;\n</code></pre>"},{"location":"Homework3/Part5/#result","title":"Result","text":"<p>The following figures are two scenes rendered with 2048 samples per pixel, a max ray depth of 5 and 1 sample per light.</p> <p>CBbunny.dae</p> <p>Render result</p> <p></p> <p>Sample rate image</p> <p></p> <p>CBdragon.dae</p> <p>Render result</p> <p></p> <p>Sample rate image</p> <p></p>"},{"location":"Homework4/Overview/","title":"Homework4: Clothsim Overview","text":""},{"location":"Homework4/Overview/#overview","title":"Overview","text":"<p>Working in progress...</p> Note for PDF <p>Some features of the website, such as picture lightbox, may not be available in the PDF version. We recommend reading the website directly for the best experience.</p>"},{"location":"Homework4/Part1/","title":"Part 1: Masses and springs","text":""},{"location":"Homework4/Part1/#results","title":"Results","text":"<p>Note: The blurriness comes from the low resolution of the screenshot. The following images show the result of scene/pinned2.json at default parameters:</p> <p></p> <p></p> <p></p>"},{"location":"Homework4/Part1/#wireframe-with-specific-conditions","title":"Wireframe with specific conditions","text":"<p>Without any shearing constraints</p> <p></p> <p></p> <p>With only shearing constraints</p> <p></p> <p></p> <p>With all constraints</p> <p></p> <p></p>"},{"location":"Homework4/Part2/","title":"Part 2: Simulation via numerical integration","text":"<p>Note: The units of the following data are all default units.</p>"},{"location":"Homework4/Part2/#changing-the-spring-constant-ks","title":"Changing the spring constant ks","text":"<p>ks=50000</p> <p></p> <p>ks=5000</p> <p></p> <p>ks=500</p> <p></p> <p>Description:  All these pictures are taken when the cloth becomes static. As ks increases, the spring becomes more difficult to extend and compress. This will cause the fabric to feel tighter. In the picture, it is obvious that the lowest point in the middle of the fabric has been decreasing, and the wrinkles have also increased. Not only that, the acceleration of the cloth will also increase (in the same posture).</p>"},{"location":"Homework4/Part2/#changing-the-density","title":"Changing the density","text":"<p>density=150</p> <p></p> <p>density=15</p> <p></p> <p>density=1.5</p> <p></p> <p>Description: All these pictures are taken when the cloth becomes static. The effect of changing this parameter is the same as changing ks (in the case of Part 2, i.e. collisions are not considered). Because density is proportional to gravity, gravity is equal to the change of the spring times ks. Therefore, as the density increases, the center becomes lower and lower, and there are more and more wrinkles. Not only that, the acceleration of the cloth will also increase (in the same posture)</p>"},{"location":"Homework4/Part2/#changing-the-damping","title":"Changing the damping","text":"<p>damping approximately equal to 0.9, time= 10s</p> <p></p> <p>damping approximately equal to 0.5, time= 10s</p> <p></p> <p>damping approximately equal to 0.1, time= 10s</p> <p></p> <p>damping = 0</p> <p></p> <p>The following are two images representing arbitrary damping when the cloth is still and does not change.</p> <p></p> <p></p> <p>Description: The three pictures above are images of the cloth running from the initial state for 10 seconds. Obviously, the smaller the damping, the faster the cloth will fall. The two pictures below show that when the object is stationary, changing the damping will have no effect (after all, it is not moving). The special case is that the damping is 0. The cloth will never stop due to energy conservation.</p>"},{"location":"Homework4/Part2/#scenepinned4json-in-its-final-resting-state","title":"scene/pinned4.json in its final resting state","text":""},{"location":"Homework4/Part3/","title":"Part 3: Handling collisions with other objects","text":"<p>Note: The units of the following data are all default units.</p>"},{"location":"Homework4/Part3/#scenespherejson-in-its-final-resting-state","title":"scene/sphere.json in its final resting state","text":"<p>ks=50000</p> <p></p> <p>ks=5000</p> <p></p> <p>ks=500</p> <p></p> <p>Description: This is because as the spring constant ks increases, the spring becomes harder to extend (or compress). This results in the fabric appearing tighter. The most noticeable effect is that as ks increases, the lowest point of the fabric rises.</p>"},{"location":"Homework4/Part3/#shaded-cloth-lying-peacefully-at-rest-on-the-plane","title":"Shaded cloth lying peacefully at rest on the plane","text":""},{"location":"Homework4/Part4/","title":"Part4: Handling self-collisions","text":""},{"location":"Homework4/Part4/#implementation","title":"Implementation","text":"<p>To generate a hash value for each point mass, we partitioned the space according to the instructions. To make the hash value unique, we scale the coordinate in each dimension by a factor that is larger than the maximum partition number of all dimensions. The space is partitioned by \\(w * h * t\\), where \\(t = max(w,h)\\), so the multiplication factor is \\(t\\). The hash value is calculated as follows:</p> C++<pre><code>  // compute a unique float identifier\n  return box_position.x * t * t + box_position.y * t + box_position.z;\n</code></pre> <p>Besides choosing the float identifier, the rest process of handling self-collisions is the same as the instruction.</p>"},{"location":"Homework4/Part4/#results","title":"Results","text":"<p>The following images show the result at default parameters:</p> <p> </p> <p>Then we tested the simulation with different densities and spring constant.</p> Note for PDF <p>The following GIFs may not be displayed correctly in the PDF version. We recommend reading the website directly for the best experience.</p> <p>Density = 150, ks = 5000 &amp; Density = 1.5, ks = 5000</p> <p> </p> <p>Density = 15, ks = 50000 &amp; Density = 15, ks = 500</p> <p> </p> <p>Density = 150, ks = 50000 &amp; Density = 1.5, ks = 500</p> <p> </p> <p>By varying the density and the spring constant the following effects can be observed:</p> <ul> <li>Density: When density is particularly high, the cloth looks squished and the space between layers of cloth is very small, compared to the normal setting. When density is very low, the space between layers of cloth is larger, and the cloth looks weaker to bend.</li> <li>Spring Constant: When the spring constant is high, the internal force causes the cloth to better maintain its shape, quite similar to the effect of low density. When the spring constant is low, the cloth looks weaker to bend, and the space between layers of cloth is also smaller.</li> <li>Total effect: It is noticeable that as long as the multiplier of density and spring constant remains the same, the cloth will have similar behavior. For example, when the density is 150 and the spring constant is 5000, the cloth behaves similarly to when the density is 1.5 and the spring constant is 5000.</li> </ul>"},{"location":"Homework4/Part5/","title":"Part5: Shaders","text":""},{"location":"Homework4/Part5/#task-1-shader-program-overview","title":"Task 1: Shader program overview","text":"<p>Shader programs execute on the GPU, run in parallel, and are written in GLSL in this assignment. These programs cover several parts of the graphic pipeline. In this assignment, two types of shaders are used: vertex and fragment shaders. The vertex shader is responsible for transforming the vertices of the object, while the fragment shader is responsible for coloring the pixels of the object.</p> <p>The vertex shader takes the vertex position, normal, and texture coordinates as input and transforms the vertex position to the screen space. It may also change the normal and texture coordinates to achieve different effects, such as bump mapping. The processed information will be passed to the fragment shader for further processing.</p> <p>The fragment shader takes the interpolated vertex position, normal, and texture coordinates as input and colors the pixel based on the lighting model and texture mapping. It will be automatically linked to the corresponding vertex shader program if there is one. Together, these two shaders form a shader program that renders objects will different materials and lighting effects.</p> <p>This flowchart shows the general process of the shader program: <pre><code>flowchart TB\n  A[Application];\n  B[Vertex Shader];\n  C[Fragment Shader];\n  D[Screen Output];\n  A --&gt;|Vertex Data| B;\n  B --&gt;|Intepolated Data| C;\n  C --&gt;|Pixel Color| D;</code></pre></p>"},{"location":"Homework4/Part5/#task-2-bling-phong-shading","title":"Task 2: Bling Phong Shading","text":""},{"location":"Homework4/Part5/#methodology","title":"Methodology","text":"<p>In this part, we implement the Bling Phong shading model in the fragment shader. The Bling Phong shading model consists of three components: ambient, diffuse, and specular. </p> \\[ L = L_\\text{ambient} + L_\\text{diffuse} + L_\\text{specular} \\] <p>The ambient component is the light that is reflected by the object from the environment. Bling-Phong shading assumes that the ambient light is constant for each part of the object. The ambient component is calculated by multiplying the ambient coefficient, i.e., how much ambient light is reflected, with the ambient light intensity:</p> \\[ L_\\text{ambient} = k_\\text{ambient} \\cdot I_\\text{ambient} \\] <p>The diffuse component is the diffuse reflection of the object. It is equal to the total irradiance arriving at the surface. Given the radiosity \\(I\\) of the light source at unit distance, the radiosity of the light source at distance \\(r\\) is \\(I/r^2\\). However, according to Lambert's cosine law, the irradiance is proportional to the cosine of the angle between the light source and the normal of the object. Therefore, the diffuse component is calculated as follows:</p> \\[ L_\\text{diffuse} = k_\\text{diffuse} \\cdot \\left(I / r^2 \\right) \\cdot \\max(0, \\vec{L} \\cdot \\vec{N}) \\] <p>The specular component is the specular reflection of the object. Specular reflection can only be observed within a certain range of angles. In Bling-Phong shading, a half-vector is used to calculate the specular reflection. The half-vector is the vector that bisects the angle between the light source direction \\(\\vec{l}\\) and the eye direction \\(\\vec{v}\\). The angle between the half-vector and the normal vector \\(\\vec{n}\\) is used to calculate the degradation of the specular reflection:</p> \\[ cos(\\theta)^{p} = \\frac{\\vec{h} \\cdot \\vec{n}}{\\left\\| \\vec{h} \\right\\| \\cdot \\left\\| \\vec{n} \\right\\|} \\] <p>Where \\(p\\) is the shininess coefficient. The specular component is calculated as follows:</p> \\[ L_\\text{specular} = k_\\text{specular} \\cdot I \\cdot \\max(0, cos(\\theta))^p \\] <p>By adjusting the ambient, diffuse, and specular coefficients, different materials can be simulated.</p>"},{"location":"Homework4/Part5/#implementation","title":"Implementation","text":"<p>In our implementation, we manually choose a set of Bling-Phong shading parameters as follows:</p> C++<pre><code>  // define ka, kd, ks, la, and p\n  float ka = 0.1; // Ambient reflection coefficient\n  float kd = 0.8; // Diffuse reflection coefficient\n  float ks = 0.5; // Specular reflection coefficient\n  vec3 la = vec3(1.0, 1.0, 1.0); // Ambient light intensity\n  float p = 32.0; // Shininess\n</code></pre> <p>First, the output is initialized as a zero vector and the ambient, diffuse and specular component is added, respectively:</p> C++<pre><code>  out_color.xyz = vec3(0.0, 0.0, 0.0);\n\n  // ambient reflection\n  out_color.xyz += ka * la;\n\n  // diffuse reflection\n  float r = length(u_light_pos - v_position.xyz);\n  vec3 l = normalize(u_light_pos - v_position.xyz);\n  vec3 n = normalize(v_normal.xyz);\n  out_color.xyz += kd * u_light_intensity / (r * r) * max(dot(n, l), 0.0);\n\n  // specular reflection\n  vec3 v = normalize(u_cam_pos - v_position.xyz);\n  vec3 h = normalize(l + v);\n  out_color.xyz += ks * u_light_intensity / (r * r) * pow(max(dot(n, h), 0.0), p);\n\n  out_color.a = 1;\n</code></pre>"},{"location":"Homework4/Part5/#results","title":"Results","text":"<p>The following images show the result of Bling Phong shading with ambient only, diffuse only, specular only, and all components combined.</p> <p>Ambient Only and Diffuse Only</p> <p> </p> <p>Specular Only and All Components</p> <p> </p>"},{"location":"Homework4/Part5/#task-3-texture-mapping","title":"Task 3: Texture Mapping","text":"<p>In this part, we implement texture mapping in the fragment shader:</p> C++<pre><code>  out_color = texture(u_texture_1, v_uv);\n</code></pre> <p>The texture is replaced by an image downloaded from Berkeley Reddit:</p> <p></p> <p>The following image shows the result of texture mapping:</p> <p></p>"},{"location":"Homework4/Part5/#task-4-bump-mapping-and-displacement-mapping","title":"Task 4: Bump Mapping and Displacement Mapping","text":""},{"location":"Homework4/Part5/#implementation-of-bump-mapping","title":"Implementation of Bump Mapping","text":"<p>In this section, we implemented Bump Mapping in the fragment shader by sampling a chosen height map. </p> <p>First, a local space disturbance vector is created by sampling the height map:</p> C++<pre><code>  // calculate dU, dV\n  float dU = (h(vec2(v_uv.x + 1.0 / u_texture_2_size.x, v_uv.y)) - h(v_uv)) * u_normal_scaling * u_height_scaling;\n  float dV = (h(vec2(v_uv.x, v_uv.y + 1.0 / u_texture_2_size.y)) - h(v_uv)) * u_normal_scaling * u_height_scaling;\n\n  // calculate local space normal\n  vec3 n0 = vec3(-dU, -dV, 1.0);\n</code></pre> <p>Then, a \\(TBN\\) matrix is created ahead of time to convert local space vectors to object space:</p> C++<pre><code>  // calculate local space tangent\n  vec3 t = v_tangent.xyz;\n\n  // calculate local space bitangent\n  vec3 n = v_normal.xyz;\n  vec3 b = cross(n, t);\n\n  // calculate TBN matrix\n  mat3 TBN = mat3(t, b, n);\n</code></pre> <p>Finally, the normal is transformed from local space to object space:</p> C++<pre><code>  // calculate normal in world space\n  vec3 nd = TBN * n0;\n</code></pre> <p>This normal is then used in the Bling Phong shading model to calculate the final color.</p>"},{"location":"Homework4/Part5/#implementation-of-displacement-mapping","title":"Implementation of Displacement Mapping","text":"<p>Different from Bump Mapping, in displacement mapping the coordinates of the input vector is also modified in the vertex shader:</p> C++<pre><code>  v_position = u_model * in_position + v_normal * h(v_uv) * u_height_scaling;\n</code></pre> <p>Where \\(h(v_uv)\\) is the height map function. The vertex position is moved along the normal direction by the height map value.</p>"},{"location":"Homework4/Part5/#results_1","title":"Results","text":"<p>A height map from Wikipedia is used in this part:</p> <p></p> <p>The following images show the result of bump mapping and displacement mapping:</p> <p>Bump Mapping with Coarseness 16 and 128</p> <p> </p> <p>Displacement Mapping with Coarseness 16 and 128</p> <p> </p> <p>According to the figures, the sphere with bump mapping has a smooth round shape despite the rough surface caused by the height map. Different from bump mapping, the sphere with displacement mapping has a rough surface and the shape is distorted. This is because the vertex position is modified in the vertex shader, which changes the shape of the object.</p> <p>Coarseness also affects the result of bump mapping and displacement mapping. The results with less coarseness have more concentrated and brighter specular reflections, while the highlights are more diffused with more coarseness.</p>"},{"location":"Homework4/Part5/#task-5-environment-mapping","title":"Task 5: Environment Mapping","text":""},{"location":"Homework4/Part5/#methodology-and-implementation","title":"Methodology and Implementation","text":"<p>In this part, we simulate mirror reflection by casting a ray from the camera position to the object and then reflecting it to the environment map.</p> <p>Given the camera position and fragment position, the eye ray direction is calculated as follows: C++<pre><code>  // Calculate the eye ray direction\n  vec3 eye_ray_dir = normalize(v_position.xyz - u_cam_pos);\n</code></pre></p> <p>The reflected ray direction is calculated with the help of object normal: C++<pre><code>  // Calculate the reflection direction\n  vec3 reflection_dir = reflect(eye_ray_dir, normalize(v_normal.xyz));\n</code></pre></p> <p>Finally, the environmental map is sampled with the same technique in previous tasks: C++<pre><code>  // sample the environment map\n  out_color = texture(u_texture_cubemap, reflection_dir);\n\n  out_color.a = 1;\n</code></pre></p>"},{"location":"Homework4/Part5/#results_2","title":"Results","text":"<p>The image below shows the result of environment mapping. The object is a sphere with a mirror reflection. The environment map is a skybox with a mountain view.</p> <p></p>"}]}